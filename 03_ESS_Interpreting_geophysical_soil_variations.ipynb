{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ESS: Analysing and interpreting FDEM data - PC class 2\n",
    "---\n",
    "# 0 - Introduction\n",
    "To evaluate to which extent the collected FDEM data can help predict the spatial variability of your group's target properties, it is important to understand the relationship between the sensor output, and the targeted properties. In this notebook, you will be able to explore the underlying pedophysical relationships between the bulk electrical conductivity of a soil sample, and specific soil properties.\n",
    "\n",
    "In a final segment of the notebook, you can develop stochastic models that can help predict the target property you have to explore.\n",
    "\n",
    "In the first code cells, you once again import the required modules, as well as some basic functions that were deployed in the previous notebooks that you can use here as well.\n",
    "In **_code cell 0.2_** the different available datasets are imported. Here you can also import your own datasets (such as inversion results etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages as needed and import\n",
    "# -------------------------------------------- #\n",
    "\n",
    "import subprocess\n",
    "import importlib\n",
    "import sys\n",
    "\n",
    "# List of required packages\n",
    "required_packages = [\n",
    "    'IPython',\n",
    "    'ipywidgets',\n",
    "    'numpy',\n",
    "    'pandas',\n",
    "    'matplotlib',\n",
    "    'scipy',\n",
    "    'sklearn',\n",
    "    'geopandas',\n",
    "    'rasterio',\n",
    "    'shapely',\n",
    "    'emagpy',\n",
    "    'requests'\n",
    "]\n",
    "\n",
    "def check_and_install_packages(package_list):\n",
    "# Function to check for required packages and install them if not found\n",
    "# integrates functionality to operate in Jupyter environment (including \n",
    "# Google Colab) or standard IDE (VSCode, PyCharm, etc.)\n",
    "# ==================================================================== #\n",
    "    for package in package_list:\n",
    "        try:\n",
    "            importlib.import_module(package)\n",
    "        except ImportError:\n",
    "            print(f\"{package} not found, installing...\")\n",
    "            try:\n",
    "                # Check if in a Jupyter (IPython) environment\n",
    "                if 'get_ipython' in globals():\n",
    "                    print(\"Using Jupyter magic command to install.\")\n",
    "                    get_ipython().system(f'pip install {package}')\n",
    "                else:\n",
    "                    # Fallback to standard IDE installation method\n",
    "                    subprocess.run(\n",
    "                        [sys.executable, '-m', 'pip', 'install', package], \n",
    "                        check=True, \n",
    "                        capture_output=True\n",
    "                        )\n",
    "            except Exception as e:\n",
    "                print(f\"{package} not installed: {e}\")\n",
    "            # Try importing the package again after installation\n",
    "            importlib.import_module(package)\n",
    "\n",
    "check_and_install_packages(required_packages)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "0.0 Import the required modules to run all code in this notebook.\n",
    "    There is redundancy in the imports, but this is to ensure that all code\n",
    "    can be run without having to worry about missing modules. Even if you \n",
    "    combine the notebook with cells from the first practicum.\n",
    "'''\n",
    "# General utility modules\n",
    "import os\n",
    "import warnings\n",
    "import re\n",
    "import requests\n",
    "\n",
    "# Modules for geopunt data visualisation\n",
    "from IPython.display import HTML\n",
    "from IPython.core.display import display\n",
    "from ipywidgets import widgets, HBox, interact, FloatSlider\n",
    "\n",
    "# Data visualisation, manipulation, and analysis\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.path\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "from scipy.spatial import cKDTree\n",
    "import scipy.stats\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.interpolate import griddata\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from scipy.optimize import root\n",
    "\n",
    "# Geospatial data manipulation and raster operations\n",
    "import geopandas as gpd\n",
    "import rasterio\n",
    "from rasterio.transform import from_origin\n",
    "from rasterio.warp import calculate_default_transform, reproject, Resampling\n",
    "from scipy.ndimage import gaussian_filter\n",
    "\n",
    "from shapely.geometry import Point\n",
    "\n",
    "# EMI 1D inversion package (emagpy)\n",
    "from emagpy import Problem\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "0.1 Import datasets\n",
    "*******************\n",
    "\n",
    "Here you can import the datasets that you will use in the notebook. The datasets\n",
    "consist of field data collected during the ESS2024 field campaign, which are \n",
    "provided via the URLs below. \n",
    "You can also import datasets that you have generated in previous notebooks. \n",
    "For those you can indicate the location on google drive (with Google Colab), or\n",
    "on your local machine (with Jupyter Notebook/your own IDE).\n",
    "\n",
    "'''\n",
    "# Location of datasets generated in PC class 1 (02_ESS-... .ipynb)\n",
    "FDEM_inverted_all = [] #dataset location Google Drive or local machine (string)\n",
    "FDEM_inverted_samples = [] #dataset location Google Drive or local machine (string)\n",
    "\n",
    "# Location of field datasets\n",
    "EC_logs = 'https://users.ugent.be/~pjdsmedt/ESS2024/EC_logs_2024.csv'\n",
    "samples = 'https://users.ugent.be/~pjdsmedt/ESS2024/samples_2024.csv' \n",
    "\n",
    "# **************************************************************************** #\n",
    "\n",
    "# Provided dataset locations\n",
    "FDEM_surveydata = 'https://users.ugent.be/~pjdsmedt/ESS2024/FDEM_2024.csv'\n",
    "FDEM_transect = 'https://users.ugent.be/~pjdsmedt/ESS2024/FDEM_transect_2024.csv'\n",
    "\n",
    "\n",
    "if FDEM_inverted_samples == []:\n",
    "    FDEM_inverted_samples = 'https://users.ugent.be/~pjdsmedt/ESS2024/FDEM_inv_samples_example.csv'\n",
    "    warnings.warn('No dataset provided for FDEM_inverted_samples. Using example dataset instead.')\n",
    "if FDEM_inverted_all == []:\n",
    "    FDEM_inverted_all = 'https://users.ugent.be/~pjdsmedt/ESS2024/FDEM_inverted_all_example.csv'\n",
    "    warnings.warn('No dataset provided for FDEM_inverted_all. Using example dataset instead.')\n",
    "\n",
    "# URL for grid masking file\n",
    "blank_json = 'https://users.ugent.be/~pjdsmedt/ESS2024/blank.json'\n",
    "\n",
    "# Create dataframes from datasets\n",
    "'''\n",
    "Import datasets as dataframes\n",
    "-----------------------------\n",
    "    - df = dataframe with the full FDEM dataset\n",
    "    - dt = dataframe with the FDEM transect\n",
    "    - ds = datasframe with the sample data (including analytical data)\n",
    "    - blank = geojson (polygon) outlining survey extent\n",
    "'''\n",
    "df = pd.read_csv(FDEM_surveydata, sep=',', header=0)\n",
    "dt = pd.read_csv(FDEM_transect, sep=',', header=0)\n",
    "ds = pd.read_csv(samples, sep=',', header=0)\n",
    "d_inv = pd.read_csv(FDEM_inverted_all, sep=',', header=0)\n",
    "d_inv_samps = pd.read_csv(FDEM_inverted_samples, sep=',', header=0)\n",
    "d_ec_logs = pd.read_csv(EC_logs, sep=',', header=0)\n",
    "blank_in = gpd.read_file(blank_json)\n",
    "blank = blank_in.to_crs('EPSG:31370')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "0.2 Basic functions\n",
    "'''\n",
    "\n",
    "# 0.2a Functions for interpolating and exporting data\n",
    "# ----------------------------------------------------\n",
    "\n",
    "def interpolate(x, y, z, cell_size, method='nearest', \n",
    "                smooth_s = 0, blank=blank):\n",
    "    \"\"\"\n",
    "    Interpolate scatter data to regular grid through selected interpolation \n",
    "    method (with scipy.interpolate for simple interpolation).\n",
    "\n",
    "    The output of this function is a Numpy array that holds the interpolated \n",
    "    data (accessed via `datagrid['grid']` in the cell below), alongside \n",
    "    the grid's cell size (`datagrid['cell_size']`) and the grid extent \n",
    "    (`datagrid['extent']`). The cell size and extent are needed to allow \n",
    "    exporting the interpolated data efficiently to a GeoTIF that can be \n",
    "    opened in any GIS software such as QGIS. \n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x : np.array\n",
    "        Cartesian GPS x-coordinates.\n",
    "\n",
    "    y : np.array\n",
    "        Cartesian GPS y-coordinates.\n",
    "\n",
    "    z : np.array\n",
    "        Data points to interpolate.\n",
    "\n",
    "    cell_size : float\n",
    "        Grid cell size (m).\n",
    "\n",
    "    method : str, optional\n",
    "        Scipy interpolation method ('nearest', 'linear', 'cubic' or 'IDW')\n",
    "\n",
    "    smooth_s : float, optional\n",
    "        Smoothing factor to apply a Gaussian filter on the interpolated grid.\n",
    "        If 0, no smoothing is performed. (Applying smoothing can result \n",
    "        in a loss of detail in the interpolated grid.)\n",
    "\n",
    "    blank : object\n",
    "        A blank object to mask (clip )interpolation beyond survey bounds.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    grid : np.array\n",
    "        Array of interpolated and masked grid containing:\n",
    "        - the interpolated grid (grid['grid'])\n",
    "        - the grid cell size (grid['cell_size'])\n",
    "        - the grid extent (grid['extent'])\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Define an evenly spaced grid over which the dataset values have to be \n",
    "    # interpolated\n",
    "    x_min = x.min()\n",
    "    x_max = x.max() + cell_size\n",
    "    y_min = y.min()\n",
    "    y_max = y.max() + cell_size\n",
    "    x_vector = np.arange(x_min, x_max, cell_size)\n",
    "    y_vector = np.arange(y_min, y_max, cell_size)\n",
    "    extent = (x_vector[0], x_vector[-1], y_vector[0], y_vector[-1])\n",
    "\n",
    "    xx, yy = np.meshgrid(x_vector, y_vector)\n",
    "    nx, ny = xx.shape\n",
    "    coords = np.concatenate((xx.ravel()[np.newaxis].T, \n",
    "                        yy.ravel()[np.newaxis].T), \n",
    "                        axis=1)\n",
    "\n",
    "    # Create a mask to blank grid outside surveyed area\n",
    "    boolean = np.zeros_like(xx)\n",
    "    boundaries = np.vstack(blank.loc[0, 'geometry'].exterior.coords.xy).T\n",
    "    bound = boundaries.copy()\n",
    "    boolean += matplotlib.path.Path(\n",
    "        bound).contains_points(coords).reshape((nx, ny))\n",
    "    boolean = np.where(boolean >= 1, True, False)\n",
    "    mask = np.where(boolean == False, np.nan, 1)\n",
    "    binary = np.where(boolean == False, 0, 1)\n",
    "    \n",
    "    # Fast (and sloppy) interpolation (scipy.interpolate)\n",
    "    if method in ['nearest','cubic', 'linear']:\n",
    "        # Interpolate \n",
    "        data_grid = griddata(\n",
    "            np.vstack((x, y)).T, z, (xx, yy), method=method\n",
    "            ) * mask\n",
    "    else:\n",
    "        print('define interpolation method')\n",
    "    \n",
    "    if smooth_s > 0:\n",
    "        data_grid = gaussian_filter(data_grid, sigma=smooth_s)\n",
    "\n",
    "    # Create a structured array with additional fields for coordinates and cell size\n",
    "    dtype = [\n",
    "        ('grid', data_grid.dtype, data_grid.shape),\n",
    "        ('cell_size', float),\n",
    "        ('extent', [\n",
    "            ('x_min', float), \n",
    "            ('x_max', float), \n",
    "            ('y_min', float), \n",
    "            ('y_max', float)\n",
    "            ])\n",
    "    ]\n",
    "    grid = np.array((data_grid, cell_size, extent), dtype=dtype)\n",
    "    \n",
    "    return grid\n",
    "  \n",
    "\n",
    "# Function to export an interpolated grid as a geotif.\n",
    "# -------------------------------------------------------\n",
    "\n",
    "def export_grid(grid_in, filename='georaster'): #TODO amend docstring\n",
    "    \"\"\"\n",
    "    Interpolate scatter data to regular grid through selected interpolation \n",
    "    method (with scipy.interpolate for simple interpolation).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    grid_in : np.array\n",
    "        Array of interpolated and masked grid.\n",
    "\n",
    "    filename : str, optional\n",
    "        Name of the GeoTIFF (.tif) file (standard = 'gridded').\n",
    "\n",
    "    y : np.array\n",
    "        Cartesian GPS y-coordinates.\n",
    "\n",
    "    z : np.array\n",
    "        Data points to interpolate.\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    #   Get grid properties\n",
    "    cell_size = grid_in['cell_size']\n",
    "    extent = grid_in['extent']\n",
    "    transform = from_origin(extent['x_min'], extent['y_min'], \n",
    "                                cell_size, -cell_size)\n",
    "\n",
    "    #   Prepare rasterio grid\n",
    "    grid_exp = grid_in['grid']\n",
    "    grid_exp[np.isnan(grid_exp)] = -99999\n",
    "    grid_exp = grid_exp.astype(rasterio.float32)\n",
    "    nx, ny = grid_exp.shape\n",
    "    grid_exp = np.flip(grid_exp, axis=0)\n",
    "\n",
    "    \n",
    "    #   Create an empty grid with correct name and coordinate system\n",
    "    with rasterio.open(\n",
    "        filename + '.tif',\n",
    "        mode='w',\n",
    "        driver='GTiff',\n",
    "        height=nx,\n",
    "        width=ny,\n",
    "        count=1,\n",
    "        dtype=str(grid_exp.dtype),\n",
    "        crs='EPSG:31370', #Lambert 1972 coordinates\n",
    "        transform=transform,\n",
    "        nodata=-99999\n",
    "    ) as dst:\n",
    "        dst.write(grid_exp, 1)\n",
    "\n",
    "    # Open the GeoTIFF file in read/write mode to flip the image vertically\n",
    "    with rasterio.open(filename + '.tif', mode='r+') as dst:\n",
    "        data = dst.read()\n",
    "        dst.write(data[0, ::-1], 1)\n",
    "\n",
    "# 0.2b Functions for evaluating LIN depth sensitivity\n",
    "# ----------------------------------------------------\n",
    "\n",
    "def lin_sens(geometry, all=False):\n",
    "    \"\"\"\n",
    "    Calculate approximative cumulative and relative sensitivities based on\n",
    "    Keller & Frischknecht, 1966 and McNeill, 1980\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    geometry : str\n",
    "        coil geometry identifier, combining orientation ('HCP' or 'PRP'),\n",
    "        and Tx-Rx separation. 'HCP0.5'is thus a HCP orientation with a 0.5 m\n",
    "        coil separation.\n",
    "\n",
    "    all : boolean, optional\n",
    "        if set to true, the function returns the cumulative and relative\n",
    "        sensitivity of the QP (ECa) and IP response of the evaluated coil pair.\n",
    "        If set to false, only the relative sensitivity of the QP response is \n",
    "        returned.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    rsens_QP : np.array\n",
    "        Array of relative QP sensitivities obtained over the evaluated depths.\n",
    "    \n",
    "    csens_QP : np.array, optional\n",
    "        Array of cumulative QP sensitivities obtained over the evaluated depths.\n",
    "\n",
    "    rsens_IP : np.array, optional\n",
    "        Array of relative IP sensitivities obtained over the evaluated depths.\n",
    "    \n",
    "    csens_IP : np.array, optional\n",
    "        Array of cumulative IP sensitivities obtained over the evaluated depths.\n",
    "\n",
    "    \"\"\"\n",
    "    # determine depth extent along which to evaluate sensitivity\n",
    "    depths = np.linspace(.0, 2.0, 100)\n",
    "    sensor_height = 0.165\n",
    "    if 'inph' in geometry:\n",
    "        coil_spacing = float(geometry[3:6])\n",
    "    else: \n",
    "        coil_spacing = float(geometry[-3:])\n",
    "\n",
    "    # create empty arrays\n",
    "    csens_QP = np.empty_like(depths)\n",
    "    rsens_QP = np.empty_like(depths)\n",
    "    csens_IP = np.empty_like(depths)\n",
    "    rsens_IP = np.empty_like(depths)\n",
    "    \n",
    "    depth_ratio = (depths + sensor_height) / coil_spacing\n",
    "    if 'HCP' in geometry:\n",
    "        csens_IP = (1 - 8 * depth_ratio ** 2) / ((4 * (depth_ratio ** 2) + 1) ** (5 / 2))\n",
    "        rsens_IP = 12 * depth_ratio * (3 - 8 * depth_ratio ** 2) / (coil_spacing * ((4 * depth_ratio ** 2) + 1) ** (7 / 2))\n",
    "        csens_QP = 1 / (4 * (depth_ratio ** 2) + 1) ** 0.5\n",
    "        rsens_QP = 4 * depth_ratio / (coil_spacing * (4 * depth_ratio ** 2 + 1) ** (3 / 2))\n",
    "\n",
    "    if 'PRP' in geometry:\n",
    "        csens_IP = (6 * depth_ratio) / ((4 * (depth_ratio ** 2) + 1) ** 2.5)\n",
    "        rsens_IP = -(96 * (depth_ratio ** 2) - 6) / (coil_spacing * (4 * (depth_ratio ** 2) + 1) ** (7 / 2))\n",
    "        csens_QP = 1 - (2 * depth_ratio) / ((4 * depth_ratio ** 2) + 1) ** 0.5\n",
    "        rsens_QP = 2 / ((coil_spacing * (4 * depth_ratio ** 2) + 1) ** (3 / 2))\n",
    "    if all:\n",
    "        return rsens_QP, csens_QP, rsens_IP, csens_IP\n",
    "    else:\n",
    "        return rsens_QP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 - Comparing FDEM data to analytical information at sampling locations\n",
    "In **_code cell 1.0_** you can plot data of a specific coil pair, along with its cumulative sensitivities. \n",
    "In addition, in the third plot you can compare sample data (such as clay content, bulk density, ...) to ECa data collected at those locations with the different coil pairs. This way, you can already visually inspect the correlation between the recorded ECa values and the sampled soil properties.\n",
    "\n",
    "In **_code cell 1.1_** you can compare the outcomes of your inversion at the sampling locations to the data from the EC logs.\n",
    "\n",
    "The columns containing soil sample data in the ds (and d_inv_samps) dataframe and their units are presented in Table 1. Columns are only shown for samples collected at 10 cm depth, as units and datatypes are similar for 50 cm depth data.\n",
    "> \n",
    ">|Column name|unit|datatype|\n",
    ">|-----------|--------|--------|\n",
    ">| *pH_H2O* | [-]|  pH water |\n",
    ">| *bd_10cm* | [g/cm^3]|bulk density |\n",
    ">| *vwc_10cm* | [%]| volumetric water content | \n",
    ">| *por_10cm* | [-]| porosity |\n",
    ">| *CEC_10cm* | [meq/100g]| cation exchange capacity |\n",
    ">| *clay_10cm* | [%]| clay content |\n",
    ">| *silt_10cm* | [%]| silt content |\n",
    ">| *sand_10cm* | [%]| sand content |\n",
    ">| *TOC_10cm* | [g/kg]| total organic carbon |\n",
    ">| *vmc_hydra* | [%]|  HydraProbe volumetric water content |\n",
    ">| *EC_bulk_hydra* |[mS/m]|  HydraProbe bulk soil electrical conductivity |\n",
    ">| *EC_water_hydra* | [mS/m]| HydraProbe soil water electrical conductivity |\n",
    ">\n",
    "> *Table 1: overview of sample data types.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.0: Evaluating FDEM data along the reference transect\n",
    "# ------------------------------------------------------\n",
    "\n",
    "\"\"\"\n",
    "Plotting the relative sensitivity of the QP (LIN ECa) response of the \n",
    "deployed coil configurations. \n",
    "------------------------------------------------------------------------\n",
    "For the third plot, you can evaluate the analytical data at the sampling \n",
    "locations (sample_col), and compare these to a selected FDEM dataset (fdem_col)\n",
    "You can select the desired variables in the two lines of code below this \n",
    "comment. You can change these strings to visually evaluate the relationships\n",
    "# between the analysed properties and the output from different coil pairs.\n",
    "\"\"\"\n",
    "\n",
    "sample_col = 'clay_10cm' # change into, e.g., 'clay_10cm [%]' or 'TOC_10cm  [g/cm3]'\n",
    "fdem_col = 'HCP0.5' # change into, for instance, 'PRP1.0' or 'HCP2.0_inph'\n",
    "\n",
    "# The following code assumes that the fdem_col is a string that matches one of\n",
    "# the datasets in the full FDEM survey dataset. If you are comparing other \n",
    "# datasets the plotted survey dataset will default to the 1.0 m HCP dataset \n",
    "# unless you change the fdem_plot variable to match the dataset you want to \n",
    "# compare to.\n",
    "fdem_plot = fdem_col\n",
    "\n",
    "# ******************************************************************** #\n",
    "\n",
    "fig, axes = plt.subplots(1,3, figsize=(21,7))\n",
    "ax1 = axes[0]\n",
    "ax2 = axes[1]\n",
    "ax3 = axes[2]\n",
    "\n",
    "# Interpolate selected FDEM dataset\n",
    "if fdem_plot not in df.columns:\n",
    "    fdem_plot = 'HCP1.0'\n",
    "    warnings.warn(f'Column {fdem_col} not found in dataset. Defaulting to {fdem_plot}.')\n",
    "data_grid = interpolate(df['x'], df['y'], df[fdem_plot], cell_size=0.25)\n",
    "extent = data_grid['extent']\n",
    "\n",
    "# Set units and colormap (cmap) for either IP or ECa data\n",
    "if 'inph' in fdem_plot:\n",
    "        unit = 'IP [ppt]'\n",
    "        cmap = 'gray_r'\n",
    "else:\n",
    "        unit = 'ECa [mS/m]'\n",
    "        cmap = 'viridis_r'\n",
    "\n",
    "# plot interpolated data in left plot (ax1)\n",
    "im = ax1.imshow(data_grid['grid'], \n",
    "                origin='lower', \n",
    "                extent=(extent['x_min'],\n",
    "                        extent['x_max'],\n",
    "                        extent['y_min'],\n",
    "                        extent['y_max']),\n",
    "                cmap = 'viridis_r'\n",
    "                )\n",
    "\n",
    "# plot sample locations and label each point with the sample ID on ax1\n",
    "ax1.plot(ds['x'], ds['y'], 'ko', markersize=15, label='Samples')\n",
    "for index, row in ds.iterrows():\n",
    "    ax1.text(row['x'], row['y'], str(int(row['ID'])), \n",
    "            fontsize=10, ha='center', va='center', color='white')\n",
    "ax1.set_xlabel('x [m]')\n",
    "ax1.set_ylabel('y [m]')\n",
    "ax1.legend()\n",
    "\n",
    "# plot coil pair sensitivities in the middle plot (ax2)\n",
    "depths = np.linspace(.0, 2.0, 100)\n",
    "for col in dt.columns:\n",
    "    if col not in ['x','y','z','t','pos'] and 'inph' not in col:\n",
    "        # calculate coil pair sensitivities\n",
    "        rsens_QP, csens_QP, rsens_IP, csens_IP = lin_sens(col, all=True)\n",
    "\n",
    "        # **************************************************************\n",
    "        # YOU CAN CHANGE THE SENSITIVITY TO BE PLOTTED (plot_sens)\n",
    "        # TO rsens_QP if YOU WANT TO PLOT THE RELATIVE SENSITIVITY\n",
    "        # **************************************************************\n",
    "        plot_sens = csens_QP\n",
    "        if 'PRP' in col:\n",
    "            ax2.plot(plot_sens, depths, linestyle = 'dashed', label=col)\n",
    "        else:\n",
    "            ax2.plot(plot_sens, depths, label=col)\n",
    "\n",
    "ax2.invert_yaxis()\n",
    "ax2.xaxis.tick_top()\n",
    "ax2.xaxis.set_label_position('top')\n",
    "if plot_sens is csens_QP:\n",
    "    ax2.set_xlabel('Cumulative Sensitivity [-]')\n",
    "elif plot_sens is rsens_QP:\n",
    "    ax2.set_xlabel('Relative Sensitivity [-]')  \n",
    "else:\n",
    "    raise ValueError('Error in sensitivity selection')\n",
    "ax2.set_ylabel('Depth [m]')\n",
    "ax2.legend()\n",
    "\n",
    "# Plot relationship between sample data and FDEM data in right plot (ax3)\n",
    "scatter = ax3.plot(ds[sample_col], \n",
    "                    ds[fdem_col],\n",
    "                    'ko', \n",
    "                    markersize=15\n",
    "                    )\n",
    "\n",
    "ax3.set_xlabel(sample_col)\n",
    "if 'inph' in fdem_col:\n",
    "    ax3.set_ylabel(fdem_col + ' [ppt]')\n",
    "    ax3.set_title(f'{fdem_col} IP vs. {sample_col}.')\n",
    "else:\n",
    "    ax3.set_ylabel(fdem_col + ' [mS/m]')\n",
    "    ax3.set_title(f'{fdem_col} ECa vs. {sample_col}.')\n",
    "\n",
    "# Add labels to scatter points showing the sample ID's\n",
    "for index, row in ds.iterrows():\n",
    "    ax3.text(row[sample_col], row[fdem_col], str(int(row['ID'])), \n",
    "            fontsize=10, ha='center', va='center', color='white')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.1: Comparing inverted EC logs with downhole EC data\n",
    "# ------------------------------------------------------\n",
    "\n",
    "# ******************************************************************** #\n",
    "\n",
    "# get inverted EC data columns from 02_ESS-... .ipynb export file\n",
    "EC_cols = [col for col in d_inv_samps.columns if 'EC' in col[:2] \n",
    "           and len(col) < 8 and 'hydra' not in col]\n",
    "\n",
    "# get interface depths\n",
    "for i, col in enumerate(EC_cols):\n",
    "    if i == 0 and col != 'EC_end':\n",
    "        depth_values = [float(col[3:])]\n",
    "    elif col != 'EC_end':\n",
    "        depth_values.append(float(col[3:]))\n",
    "\n",
    "# Add the last depth value\n",
    "depth_values.append(depth_values[-1] + (depth_values[-1] - depth_values[-2]))\n",
    "\n",
    "unique_sample_ids = d_inv_samps['ID'].unique()\n",
    "\n",
    "# number of subplot rows and columns\n",
    "subplot_rows = 3\n",
    "subplot_cols = 5\n",
    "\n",
    "# Get axis limits based on conductivity values in inverted and field datasets\n",
    "global_x_min = min(d_inv_samps[EC_cols].min().min(), d_ec_logs['EC_msm'].min())\n",
    "global_x_max = max(d_inv_samps[EC_cols].max().max(), d_ec_logs['EC_msm'].max())\n",
    "\n",
    "# Create the subplots\n",
    "fig, axes = plt.subplots(subplot_rows, subplot_cols, figsize=(15, 9))\n",
    "fig.subplots_adjust(hspace=0.5, wspace=0.3)\n",
    "# Loop through each sample ID and plot the EC values\n",
    "for i, sample_id in enumerate(unique_sample_ids):\n",
    "    # Get row index for the current sample ID\n",
    "    row_index = d_inv_samps.loc[d_inv_samps['ID'] == sample_id].index[0]\n",
    "    row_data = d_inv_samps.loc[row_index, EC_cols].values\n",
    "    # Get the current subplot axis\n",
    "    ax = axes[i // subplot_cols, i % subplot_cols]\n",
    "    # Get the EC logs data for the current sample ID\n",
    "    data = d_ec_logs[d_ec_logs['ID'] == sample_id]\n",
    "    # append additional depth value that is 0.10 m deeper than last depth value\n",
    "    new_row = pd.DataFrame([{'EC_msm': data['EC_msm'].iloc[-1], \n",
    "                             'depth_m': data['depth_m'].iloc[-1] + 0.10}])\n",
    "    data = pd.concat([data, new_row], ignore_index=True)\n",
    "\n",
    "    if sample_id == 1:\n",
    "        ax.step(row_data, depth_values, label=f'Modelled EC')\n",
    "        ax.step(data['EC_msm'], data['depth_m'], label=f'EC logs')\n",
    "    else:\n",
    "        ax.step(row_data, depth_values)\n",
    "        ax.step(data['EC_msm'], data['depth_m'])\n",
    "\n",
    "    ax.set_xlabel('EC [mS/m]')\n",
    "    ax.set_ylabel('Depth [m]')\n",
    "    ax.set_title(f'Sample {int(sample_id)}')\n",
    "    ax.set_xlim(global_x_min, global_x_max)\n",
    "    ax.invert_yaxis()\n",
    "    ax.grid(True)\n",
    "fig.legend(bbox_to_anchor=(0.3, 0.95), ncol=2)\n",
    "fig.suptitle('Modelled EC profiles at sampling locations', fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 - Pedophysical modelling - predicting bulk EC\n",
    "\n",
    "Pedophysical models are models that aim to describe the relationship between the observed geophysical properties, and the soil properties and states of interest. Generally, a geophysical property of a soil (i.e., a soil being a medium that integrates multiple phases) is influenced by the soil constituents (i.e., the relative volumetric proportion of the soil components and their physical properties), and the soil structure (i.e., the spatial distribution of the soil constituents and their interconnection). \n",
    "\n",
    "### Archie's Law\n",
    "In class, we have briefly discussed Archie's law, which, when modified for soils, primarily accounts for the conductivity of the pore fluid ($\\sigma_{wat}$), and considers the degree of saturation $S_{wat}$ alongside a saturation exponent $n$, which relates to the pore structure of the medium. The degree of saturation equals the ratio of the volumetric water content ($\\theta$) to the porosity ($\\phi$) of the medium:\n",
    "\n",
    "$$\n",
    "S_{wat} = \\frac{\\theta}{\\phi}\n",
    "$$\n",
    "\n",
    "\n",
    "The combined Archie’s law is:\n",
    "\n",
    "$$\n",
    "\\sigma = \\phi^{m} \\times S_{wat}^{n} \\times \\sigma_{wat}.\n",
    "$$\n",
    "\n",
    "This allows estimating the conductivity $\\sigma$ for partially saturated media whereby $m$ (fixed at 1.5) and $n$ (fixed at 2) allow including the influence of the medium’s structure.\n",
    "____\n",
    "<br>\n",
    "\n",
    "### Waxman and Smits equation \n",
    "One major shortcoming of Archie’s law was originally developed for sandy rocks, which presents significant shortcomings when applied to other media such as soils. Most importantly, surface conduction (i.e., the conduction along the surface of dry particles) is not taken into account. In all soils, the influence of surface conductance is non-negligible, and increases with clay content. Other models account for this, such as the adjustment proposed by [Waxman & Smits (1968)](https://users.ugent.be/~pjdsmedt/ESS2023/WS_1986.pdf), who quantify the surface conductivity ($\\sigma_{surface}$) by defining a parameter $Q_V$ based on the cation exchange capacity ($CEC$):\n",
    "\n",
    "\n",
    "$$\n",
    "Q_v = pd \\frac{1-\\phi}{\\phi} \\times CEC ,\n",
    "$$\n",
    "\n",
    "with $pd$ the particle density. This results in the following relationship with bulk electrical conductivity:\n",
    "\n",
    "$$\n",
    "\\sigma = \\frac{1}{\\phi^{-2}} \\times (\\sigma_{water} + B \\times Q_v),\n",
    "$$\n",
    "\n",
    "resulting in the surface conductivity:\n",
    "\n",
    "$$\n",
    "\\sigma_{surface} = (B \\times Q_v \\times S_{wat} \\times \\phi^{m}).\n",
    "$$\n",
    "\n",
    "Hereby, $B$ represents an equivalent counterion (sodium clay-exchange cations) conductivity that is determined empirically ([Revil et al. 1998](https://agupubs.onlinelibrary.wiley.com/doi/10.1029/98JB02125)).\n",
    "___\n",
    "<br>\n",
    "\n",
    "### Linde et al. 2006\n",
    "More recently, a push towards more practical integrations of the surface conductivity\n",
    "The equation from [Linde et al. 2006](https://agupubs.onlinelibrary.wiley.com/doi/full/10.1029/2006WR005131) mentioned in [Romero-Ruiz et al. 2018](http://doi.wiley.com/10.1029/2018RG000611), integrates the surface conductivity as follows:\n",
    "\n",
    "$$\n",
    "\\sigma = \\phi^{m} \\times S_{wat}^{n} \\times \\sigma_{water} + (1 - \\phi^{m}) \\times \\sigma_{surface} .\n",
    "$$\n",
    "\n",
    "Hereby, the surface conductivity is derived from soil texture, through empirical observations, by attributing more weight to a set conductivity of solid particles as the particle size decreases.\n",
    "\n",
    "### Fu et al. 2021\n",
    "An alternative formulation of this approach was presented by [Fu et al., 2021](https://www.sciencedirect.com/science/article/pii/S0022169421002079):\n",
    "\n",
    "$$\n",
    "\\sigma = \\sigma_{water} \\times \\theta^{w} + \\theta \\times \\phi \\times \\sigma_{surface} + (1-\\phi) \\times \\sigma_{solid},\n",
    "$$\n",
    "\n",
    "with a constant $w$ that has an average values of 2, respectively. The conductivity of the solid phase, which is negligible, is included as $\\sigma_{solid}$. \n",
    "Based on empirical data, the surface conductivity $\\sigma_{surface}$ is then simplified as:\n",
    "\n",
    "$$\n",
    "\\sigma_{surface} = 0.654\\frac{clay}{sand + silt} + 0.0183 ,\n",
    "$$\n",
    "\n",
    "following [Doussan & Ruy (2009)](https://agupubs.onlinelibrary.wiley.com/doi/full/10.1029/2008WR007309). For the function presented by [Fu et al. 2021](https://www.sciencedirect.com/science/article/pii/S0022169421002079), extensive calibration was performed based on soil samples with clay content ranging from 0 to 33%.\n",
    "\n",
    "An overview of these and other pedophysical models can be found in [Mendoza et al. (2024)](https://arxiv.org/pdf/2403.07473.pdf).\n",
    "\n",
    "The equations incorporating surface conductivity are provided in **_code cell 2.0_** as `waxsmith`, `linde`, and `fu`, respectively. \n",
    "You can test these for a range of varying soil properties in **_code cell 2.1_**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.0: Pedophysical modelling - Linde et al. 2006\n",
    "# -----------------------------------------------\n",
    "\n",
    "def waxsmits(vwc, bd, water_ec, CEC, pdn=2.65, m=1.5, n=2, a = 0.4):\n",
    "    \"\"\"        \n",
    "    Pedophysical modelling following the revised Waxman-Smits model\n",
    "    as proposed by Revil et al. 2013.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    vwc: float\n",
    "        volumetric water content [%]\n",
    "    \n",
    "    bd: float\n",
    "        bulk density [g/cm3]\n",
    "\n",
    "    CEC: float\n",
    "        cation exchange capacity [meq/100g]\n",
    "\n",
    "    water_ec: float\n",
    "        Soil water real electrical conductivity [mS/m]\n",
    "\n",
    "    pdn: float\n",
    "        particle density [g/cm3]\n",
    "\n",
    "    m: float\n",
    "        cementation exponent [-]\n",
    "\n",
    "    n: float\n",
    "        saturation exponent [-]\n",
    "\n",
    "    a: float\n",
    "        fitting parameter [-] (see Wunderlich et al. 2013)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    bulk_ec: float\n",
    "        Soil bulk real electrical conductivity [mS/m]\n",
    "    \"\"\"  \n",
    "    #Convert particle density to kg/m3\n",
    "    pdn_kg = pdn * 1000\n",
    "    water_ec = water_ec/1000 # to S/m\n",
    "    por = 1-(bd/pdn)\n",
    "    sat_w = (vwc/100)/por \n",
    "    \n",
    "    Q_v = (pdn_kg) * ((1-por) / por) * CEC\n",
    "    # formation factor\n",
    "    f_form = por**(-m) \n",
    "    # Empirical ion mobility factor (Revil et al. 1998)\n",
    "    B = (4.78e-8) * (1 - 0.6 * np.exp(-(water_ec/0.013))) \n",
    "\n",
    "    bulk_ec = (sat_w**n/f_form) / a * (water_ec + ((B * Q_v)/sat_w))\n",
    "    return bulk_ec*1000 # to milliSiemens per meter\n",
    "    \n",
    "\n",
    "\n",
    "def linde(vwc, bd, water_ec, clay, sand, pdn=2.65, m=1.5, n=2):\n",
    "    \"\"\"        \n",
    "    Pedophysical modelling following the Linde et al. 2006 model.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    vwc: float\n",
    "        volumetric water content [%]\n",
    "    \n",
    "    bd: float\n",
    "        bulk density [g/cm3]\n",
    "\n",
    "    clay: float\n",
    "        Soil volumetric clay content [%]\n",
    "\n",
    "    water_ec: float\n",
    "        Soil water real electrical conductivity [mS/m]\n",
    "\n",
    "    pdn: float\n",
    "        particle density [g/cm3]\n",
    "\n",
    "    m: float\n",
    "        cementation exponent [-]\n",
    "\n",
    "    n: float\n",
    "        saturation exponent [-]\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    bulk_ec: float\n",
    "        Soil bulk real electrical conductivity [mS/m]\n",
    "    \"\"\"  \n",
    "\n",
    "    por = 1-(bd/pdn) # porosity\n",
    "    sat_w = (vwc/100)/por # water saturation\n",
    "    f_form = por**(-m) # formation factor\n",
    "    \n",
    "    water_ec = water_ec/1000\n",
    "\n",
    "    silt = 100 - clay - sand\n",
    "\n",
    "    radius_clay = 0.002/2000\n",
    "    radius_silt = 0.025/2000\n",
    "    radius_sand = 0.75/2000\n",
    "\n",
    "    solid_ec = 1*(10**-7) # Solid electrical conductivity\n",
    "    clay_ec= 3*(solid_ec/radius_clay)  # clay electrical conductivity\n",
    "    silt_ec = 3*(solid_ec/radius_silt) # Silt electrical conductivity\n",
    "    sand_ec = 3*(solid_ec/radius_sand) # Sand electrical conductivity\n",
    "\n",
    "    surf_ec = np.average([clay_ec*(clay/100), \n",
    "                          sand_ec*(sand/100), \n",
    "                          silt_ec*(silt/100)])\n",
    "    bulk_ec = (((sat_w**n)*water_ec) \n",
    "               + (f_form - 1)*(surf_ec))/f_form \n",
    "    \n",
    "    return bulk_ec*1000\n",
    "\n",
    "def fu(vwc, bd, water_ec, clay, pdn=2.65, solid_ec = (1*(10**-7)), w=2):\n",
    "    \"\"\"\n",
    "    Pedophysical modelling following Fu et al. 2021\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    vwc : float\n",
    "        Soil volumetric water content [%].\n",
    "\n",
    "    bd : float\n",
    "        Soil bulk density [g/cm3].\n",
    "\n",
    "    water_ec : float\n",
    "        Soil water real electrical conductivity [mS/m].\n",
    "\n",
    "    clay : float\n",
    "        Soil clay content [%].\n",
    "\n",
    "    pdn : float, optional\n",
    "        Particle density [g/cm3], default is 2.65.\n",
    "\n",
    "    solid_ec : float, optional\n",
    "        Soil solid real electrical conductivity [mS/m], default is 1e-7.\n",
    "        \n",
    "    w : float, optional\n",
    "        Phase exponent of the water, default is 2.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    bulk_ec: float\n",
    "        Soil bulk real electrical conductivity [mS/m]\n",
    "\n",
    "    \"\"\"\n",
    "    d = 0.6539\n",
    "    e = 0.0183\n",
    "    vwc = vwc/100 # to m3/m3\n",
    "    water_ec  = water_ec/1000 # S/m\n",
    "    surf_ec = (d*clay/(100-clay))+e # S/m\n",
    "    por = 1-(bd/pdn) # porosity []\n",
    "\n",
    "    bulk_ec = water_ec * vwc**w + vwc * por * surf_ec + (1-por) * solid_ec\n",
    "    \n",
    "    return bulk_ec*1000\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dynamic evaluation of pedophysical models\n",
    "\n",
    "In the following two code cells you can evaluate the pedophysical models presented above dynamically, by modifying input parameter values with sliders. If needed, you can adjust the starting values and ranges for the sliders. Keep in mind that setting all slides to a maximum value can lead to unrealistic scenarios (e.g., unrealistically high bulk densities for clayey soils)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.1: Evaluate the Waxman-Smits model with Revil et al. 1998 modification\n",
    "# -------------------------------------------------------------------------\n",
    "\n",
    "\"\"\"\n",
    "In this cell, code is provided to evaluate the Wasmits model with the \n",
    "Revil et al. 1998 modification. You can modify the starting values and ranges\n",
    "for the relevant parameters (CEC, VWC, BD, ECW) to evaluate the model's outcome\n",
    "in different scenarios.\n",
    "\"\"\"\n",
    "\n",
    "# Set starting values and ranges for sliders\n",
    "min_CEC = 1; max_CEC = 50\n",
    "min_bd = 1.0; max_bd = 2.1\n",
    "min_vwc = 10; max_vwc = 50\n",
    "min_ecw = 10; max_ecw = 100   \n",
    "\n",
    "# ******************************************************************** #\n",
    "# Create empty arrays to populate with values\n",
    "iterations = 100\n",
    "CEC_i = np.linspace(min_CEC, max_CEC, iterations)\n",
    "vwc_i = np.linspace(min_vwc, max_vwc, iterations)\n",
    "b_dens_i = np.linspace(min_bd, max_bd, iterations)\n",
    "\n",
    "# Update plot function\n",
    "def update_plot(CEC, VWC, BD,ECW):\n",
    "    # Calculate model outputs\n",
    "    c_v_it = [waxsmits(vwc, BD, ECW, CEC) for vwc in vwc_i]\n",
    "    c_b_it = [waxsmits(VWC, bd, ECW, CEC) for bd in b_dens_i]\n",
    "    c_c_it = [waxsmits(VWC, BD, ECW, cec) for cec in CEC_i]\n",
    "\n",
    "    # Plotting\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "\n",
    "    axes[0].plot(vwc_i, c_v_it, linewidth=3)\n",
    "    axes[0].set_xlabel(\"volumetric water content [%]\")\n",
    "    axes[0].set_ylabel(\"bulk EC [mS/m]\")\n",
    "    axes[0].set_title(\"change bd and CEC with slider\")\n",
    "\n",
    "    # CEC and BD plot\n",
    "    axes[1].plot(b_dens_i, c_b_it, linewidth=3)\n",
    "    axes[1].set_xlabel(\"bulk density [g/cm^3]\")\n",
    "    axes[1].set_ylabel(\"bulk EC [mS/m]\")\n",
    "    axes[1].set_title(\"change vwc and CEC with slider\")\n",
    "\n",
    "\n",
    "    # CEC and BD plot\n",
    "    average_ec = np.mean(c_c_it)\n",
    "    plot_lab = 'mean bulk EC: ' + str(round(average_ec, 2)) + '[mS/m]'\n",
    "    axes[2].plot(CEC_i, c_c_it, label=plot_lab, linewidth=3)\n",
    "    \n",
    "    # set y axis range from 0 to 100 mS/m\n",
    "    axes[2].set_ylim([0, 100])\n",
    "    axes[2].set_xlabel(\"CEC [mmol/g]\")\n",
    "    axes[2].set_ylabel(\"bulk EC [mS/m]\")\n",
    "    axes[2].legend(loc='upper right')\n",
    "    fig.suptitle('Evaluation of Waxman & Smits (1968) EC model with '\n",
    "                 'Revil et al. 1998 modification', fontsize=14)\n",
    "    plt.show()\n",
    "\n",
    "# Define sliders\n",
    "CEC_slider = FloatSlider(value=min_CEC, min=min_CEC, max=max_CEC, \n",
    "                         step=0.1, description='CEC [mmol/g]')\n",
    "VWC_slider = FloatSlider(value=min_vwc, min=min_vwc, max=max_vwc, \n",
    "                         step=0.1, description='VWC [%]')\n",
    "BD_slider = FloatSlider(value=min_bd, min=min_bd, max=max_bd, \n",
    "                        step=0.01, description='BD [g/cm^3]')\n",
    "ECW_slider = FloatSlider(value=min_ecw, min=min_ecw, max=max_ecw, \n",
    "                         step=0.1, description='EC_w [mS/m]')\n",
    "\n",
    "# Use interact to create interactive widgets\n",
    "interact(update_plot, CEC=CEC_slider, VWC=VWC_slider, BD=BD_slider, ECW=ECW_slider)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.1: Evaluate the Linde et al 2016 and Fu et al. 2021 models\n",
    "# ----------------------------------------------------------------------\n",
    "\n",
    "\"\"\" \n",
    "In this cell, code is provided to evaluate the Linde et al. 2006 and Fu et al.\n",
    "2021 models. You can modify the starting values and ranges for the relevant\n",
    "parameters (clay, VWC, ECW, BD) to evaluate the model's outcome in different\n",
    "scenarios.\n",
    "\"\"\"\n",
    "\n",
    "# Set starting values and ranges for sliders\n",
    "min_clay = 5; max_clay = 40\n",
    "min_vwc = 10; max_vwc = 50\n",
    "min_ecw = 10; max_ecw = 100\n",
    "min_bd = 1.0; max_bd = 2.1\n",
    "\n",
    "# getting clay and bd range n iterations\n",
    "iterations = 100\n",
    "clay_i = np.linspace(min_clay, max_clay, iterations)\n",
    "b_dens_i = np.linspace(min_bd, max_bd, iterations)\n",
    "vwc_i = np.linspace(min_vwc, max_vwc, iterations)\n",
    "\n",
    "# Update plot function based on slider changes\n",
    "def update_plot(CLAY, VWC, ECW, BD):\n",
    "    # Iterate over bulk density for first plot\n",
    "    SAND = (100-CLAY)/2\n",
    "    linde_b_it = [linde(VWC, bd, ECW, CLAY, SAND) for bd in b_dens_i]\n",
    "    fu_b_it = [fu(VWC, bd, ECW, CLAY) for bd in b_dens_i]\n",
    "    # Iterate over clay content for second plot\n",
    "    linde_c_it = [linde(VWC, BD, ECW, clay,((100-clay)/2)) for clay in clay_i]\n",
    "    fu_c_it = [fu(VWC, BD, ECW, clay) for clay in clay_i]\n",
    "    # Iterate over VWC for third plot\n",
    "    linde_v_it = [linde(vwc, BD, ECW, CLAY, SAND) for vwc in vwc_i]\n",
    "    fu_v_it = [fu(vwc, BD, ECW, CLAY, SAND) for vwc in vwc_i]\n",
    "\n",
    "    # Plotting\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "    \n",
    "    axes[0].plot(b_dens_i, linde_b_it, linewidth=3)\n",
    "    axes[0].plot(b_dens_i, fu_b_it, linewidth=3)\n",
    "    axes[0].set_xlabel(\"bulk density [g/cm^3]\")\n",
    "    axes[0].set_ylabel(\"bulk EC [mS/m]\")\n",
    "    axes[0].set_title(\"change clay and vwc with slider\")\n",
    "\n",
    "    axes[1].plot(clay_i, linde_c_it, linewidth=3)\n",
    "    axes[1].plot(clay_i, fu_c_it, linewidth=3)\n",
    "    axes[1].set_xlabel(\"clay content [%]\")\n",
    "    axes[1].set_ylabel(\"bulk EC [mS/m]\")\n",
    "    axes[1].set_title(\"change vwc and bd with slider\")\n",
    "\n",
    "    axes[2].plot(vwc_i, linde_v_it, label='Linde et al., 2006', linewidth=3)\n",
    "    axes[2].plot(vwc_i, fu_v_it, label='Fu et al., 2021', linewidth=3)\n",
    "    axes[2].set_xlabel(\"volumetric water content [%]\")\n",
    "    axes[2].set_ylabel(\"bulk EC [mS/m]\")\n",
    "    axes[2].set_title(\"change clay and bd with slider\")\n",
    "    \n",
    "    axes[2].legend(loc='upper left')\n",
    "    fig.suptitle(f'Comparison of Linde et al. 2006 and Fu et al. 2021 for EC modelling', fontsize=14)\n",
    "    plt.show()\n",
    "\n",
    "# Define sliders for each parameter and assure that sliders are positioned below the plots\n",
    "clay_slider = FloatSlider(value=min_clay, min=min_clay, max=max_clay, step=0.1, description='Clay content [%]', style={'description_width': 'initial'}) \n",
    "vwc_slider = FloatSlider(value=min_vwc, min=min_vwc, max=max_vwc, step=0.1, description='VWC [%]', style={'description_width': 'initial'}) \n",
    "ecw_slider = FloatSlider(value=min_ecw, min=min_ecw, max=max_ecw, step=0.1, description='EC_w [mS/m]', style={'description_width': 'initial'}) \n",
    "bd_slider = FloatSlider(value=min_bd, min=min_bd, max=max_bd, step=0.01, description='BD [g/cm^3]', style={'description_width': 'initial'}) \n",
    "\n",
    "# Interactive widget\n",
    "interact(update_plot, CLAY=clay_slider, VWC=vwc_slider, ECW=ecw_slider, BD=bd_slider)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating sampling data\n",
    "\n",
    "In **_code cell 2.2_** you can evaluate the performance of these models by comparing the modelled soil EC values based on the analytical data, and comparing these to point EC data collected with the HydraProbe during fieldwork, and any of the FDEM datasets. Just specify the FDEM dataset to evaluate. You can choose the fix the conductivity on the pore solution, to evaluate what impact this has on the prediction, by setting the `fix_ec_water` variable to `True`. This will use the average value of the pore solution conductivity as measured with the HydraProbe during fieldwork.  \n",
    "\n",
    "By running **_code_cell 2.3_** you can add the EC values modelled through the Linde et al. (2006) equation to the samples dataframe (dataframe `ds`), and exporting it as a csv file to your Google drive.\n",
    "\n",
    "_**REMINDER** to check the column names for a specific pandas dataframe, just visualise the '.columns' attribute, for instance as `ds.columns` for the `ds` dataframe that holds the sample data._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.2: Evaluating pedophysical relationships at sampling locations\n",
    "# ----------------------------------------------------------------\n",
    "\n",
    "# Set the column names for EC datasets (plotted on x-axes) you want to compare \n",
    "# with the EC values predicted by the pedophysical models (plotted on y-axes)\n",
    "\n",
    "EC_col = 'EC_bulk_hydra' # Default = hydraprobe bulk EC values \n",
    "fdem_col = 'HCP0.5' # Default = 0.5 m HCP FDEM dataset. You can change this to columns of the inverted FDEM data at sampling locations\n",
    "\n",
    "# Set conductivity of the pore solution as fixed (ec_water_avg variable)'\n",
    "\n",
    "fix_ec_water = False # if False, for each sample location the EC_water value \n",
    "                     # measured by the HydraProbe is used\n",
    "ec_water_avg = d_inv_samps['EC_water_hydra'].median() # fixed EC_water [mS/m]\n",
    "\n",
    "# ******************************************************************** #\n",
    "\n",
    "# Initialize empty lists to store the predicted bulk_ec values\n",
    "predicted_bulk_ec_linde = []\n",
    "predicted_bulk_ec_fu = []\n",
    "predicted_bulk_ec_ws = []\n",
    "\n",
    "# Loop through the rows of the DataFrame\n",
    "for index, row in d_inv_samps.iterrows():\n",
    "    # Get the input values from the DataFrame\n",
    "    vmc = row['vmc_hydra']\n",
    "    bd = row['bd_10cm']\n",
    "    sand = row['sand_10cm']\n",
    "    CEC = row['CEC_10cm']\n",
    "    silt = row['silt_10cm']\n",
    "    clay = row['clay_10cm']\n",
    "    if fix_ec_water:\n",
    "        water_ec = ec_water_avg\n",
    "    else:\n",
    "        water_ec = row['EC_water_hydra']\n",
    "    \n",
    "    # Calculate the bulk electrical conductivity based on linde and fu models\n",
    "    bulk_ec_linde = linde(vmc, bd, sand, clay, water_ec)\n",
    "    bulk_ec_fu = fu(vmc, bd, water_ec, clay)\n",
    "    # waxsmits(vwc, bd, water_ec, CEC, pdn=2.65, m=1.5, n=2, a = 0.4):\n",
    "    bulk_ec_ws = waxsmits(vmc, bd, water_ec, CEC)\n",
    "\n",
    "    # Store the predicted bulk_ec values\n",
    "    predicted_bulk_ec_linde.append(bulk_ec_linde)\n",
    "    predicted_bulk_ec_fu.append(bulk_ec_fu)\n",
    "    predicted_bulk_ec_ws.append(bulk_ec_ws)\n",
    "\n",
    "# Create subplots\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=[12, 5])\n",
    "\n",
    "# Plot bulk_ec versus hydra_ec [mS/m]\n",
    "axes[0].scatter(d_inv_samps[EC_col], predicted_bulk_ec_linde, alpha=0.7)\n",
    "axes[0].scatter(d_inv_samps[EC_col], predicted_bulk_ec_fu, alpha=0.7)\n",
    "axes[0].scatter(d_inv_samps[EC_col], predicted_bulk_ec_ws, alpha=0.7, color='gray')\n",
    "\n",
    "# UNCOMMENT TO ADD A TRENDLINE TO EACH PLOT\n",
    "# # add trendline to each plot\n",
    "# slope, intercept, r_value, p_value, std_err = scipy.stats.linregress(ds['hydra_ec [mS/m]'], predicted_bulk_ec_linde)\n",
    "# line = slope * ds['hydra_ec [mS/m]'] + intercept\n",
    "# axes[0].plot(ds['hydra_ec [mS/m]'], line, label='Linde et al. 2006')\n",
    "# slope, intercept, r_value, p_value, std_err = scipy.stats.linregress(ds['hydra_ec [mS/m]'], predicted_bulk_ec_fu)\n",
    "# line = slope * ds['hydra_ec [mS/m]'] + intercept\n",
    "# axes[0].plot(ds['hydra_ec [mS/m]'], line, label='Fu et al. 2021')\n",
    "# slope, intercept, r_value, p_value, std_err = scipy.stats.linregress(ds['hydra_ec [mS/m]'], predicted_bulk_ec_ws)\n",
    "# line = slope * ds['hydra_ec [mS/m]'] + intercept\n",
    "# axes[0].plot(ds['hydra_ec [mS/m]'], line, label='Waxman-Smits/Revill')\n",
    "# axes[0].legend()\n",
    "\n",
    "axes[0].set_xlabel(\"hydra_ec [mS/m]\")\n",
    "axes[0].set_ylabel(\"Predicted EC [mS/m]\")\n",
    "#axes[0].set_title(\"Predicted EC vs hydra_ec\")\n",
    "\n",
    "# Plot bulk_ec versus selected FDEM column LIN ECa [mS/m]\n",
    "axes[1].scatter(ds[fdem_col], predicted_bulk_ec_linde, alpha=0.9, \n",
    "                label = \"Linde et al. 2006\")\n",
    "axes[1].scatter(ds[fdem_col], predicted_bulk_ec_fu, alpha=0.7, \n",
    "                label = \"Fu et al. 2021\")\n",
    "axes[1].scatter(ds[fdem_col], predicted_bulk_ec_ws, alpha=0.5, color='gray', \n",
    "                label = \"Waxman-Smits/Revill\")\n",
    "axes[1].legend()\n",
    "axes[1].set_xlabel(fdem_col)\n",
    "axes[1].set_ylabel(\"Predicted EC [mS/m]\")\n",
    "#axes[1].set_title(f\"Predicted EC vs {fdem_col}\")\n",
    "\n",
    "fig.suptitle('Comparison of observed EC values vs predicted bulk EC'\n",
    "              ' from pedophysical models', fontsize=14)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.3: Store predicted EC values in the `d_inv_samps` dataframe and save .csv\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "\"\"\"\n",
    "Here you can export the predicted bulk EC values from the pedophysical models\n",
    "to a .csv file. Per default, this will be a file that combines your inversion\n",
    "results at the sampling locations, the FDEM ECa data, the sample information,\n",
    "and the predicted bulk EC values from the Linde, Fu and Waxman-Smits models.\n",
    "\"\"\"\n",
    "\n",
    "# Define a filename for exporting\n",
    "filename = 'samples_and_predictions.csv'\n",
    "\n",
    "# ******************************************************************** #\n",
    "# Create column for predicted EC values (in mS/m) in the d_inv_samps dataframe and export as csv\n",
    "d_inv_samps['Linde EC_predict [mS/m]'] = predicted_bulk_ec_linde\n",
    "d_inv_samps['Fu EC_predict [mS/m]'] = predicted_bulk_ec_fu\n",
    "d_inv_samps['Waxman-Smits EC_predict [mS/m]'] = predicted_bulk_ec_ws\n",
    "d_inv_samps.to_csv(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Exploring relationships between target properties and observed geophysical variations.\n",
    "\n",
    "To evaluate to which extent the collected FDEM data can help predict the spatial variability of your group's target property, it is important to understand the relationship between the sensor output, and the targeted properties. This can be done at the sampling locations by further exploring the `d_inv_samps` dataframe that contains the FDEM data and analytical data at these locations, as well as your inversion results ath these locations. For instance, you can compare the FDEM ECa data to a given property at a given depth, and make the same comparision with the Hydraprobe EC data.\n",
    "\n",
    "## Stochastic modelling\n",
    "A stochastic empirical approach can be used as a basis for predictions. As an example, a regression analysis is included in **_code cell 3.1_**. \n",
    "The obtained linear function is incorporated in the 'lin_fit' object. You can obtain the function by simply printing the objects as: `print(lin_fit)`, as is done in **_code cell 3.1_**. \n",
    "\n",
    "To extract the slope and intercept from the linear model, you can directly access these as:\n",
    "\n",
    "`slope = lin_fit[1]` <br />\n",
    "`intercept = lin_fit[0]`,\n",
    "\n",
    "Suited models, that avoid overfitting and provide satisfactory prediction errors, can then be used to predict the target variable based on the obtained model. \n",
    "Alongside the regression analysis, a cross-validation is performed using the 'leave-one-out' method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.0: evaluating relationships between target properties and conductivity data\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "target = 'pH_H2O_10cm' # ds columnname for column with target property \n",
    "fdem_col = 'EC_end' # ds columnname for column with FDEM dataset (inverted or non-inverted)\n",
    "hydra_col = 'EC_bulk_hydra' # ds columnname for hydraprobe data column\n",
    "\n",
    "# create a copy of the sample dataset to plot and, if needed, filter out\n",
    "# specific points (outliers/validation data/...)\n",
    "ds_in = d_inv_samps.copy()\n",
    "\n",
    "# # uncomment these lines to remove the validation or other points \n",
    "# exclude_points = [11, 12, 13, 14, 15] # replace with point ID's you want to remove\n",
    "# ds_in = ds_in[~ds_in['ID'].isin(exclude_points)]\n",
    "\n",
    "# ******************************************************************** #\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# Plotting\n",
    "# -----------------------------------------------------------\n",
    "\n",
    "fig, ax = plt.subplots(1,2, figsize=(15,7))\n",
    "# Define different axes for plotting\n",
    "\n",
    "# FDEM compare plot\n",
    "ax[0].scatter(ds_in[fdem_col],ds_in[target], s=200)\n",
    "ax[0].set_xlabel(f'{fdem_col} data')\n",
    "ax[0].set_ylabel(f'{target}')\n",
    "# add sample ID labels to the plot\n",
    "for index, row in ds_in.iterrows():\n",
    "    ax[0].text(row[fdem_col], row[target], str(int(row['ID'])), \n",
    "    fontsize=10, ha='center', va='center', color='white')\n",
    "ax[0].grid()\n",
    "ax[0].set_title(f'{fdem_col} vs. {target}')\n",
    "\n",
    "# HyrdaGo compare plot\n",
    "ax[1].scatter(ds_in[hydra_col],ds_in[target], s=200)\n",
    "ax[1].set_xlabel(hydra_col)\n",
    "ax[1].set_ylabel(f'{target}')\n",
    "\n",
    "#   add sample ID labels to the plot\n",
    "for index, row in ds_in.iterrows():\n",
    "    ax[1].text(row[hydra_col], row[target], str(int(row['ID'])), \n",
    "    fontsize=10, ha='center', va='center', color='white')\n",
    "ax[1].grid()\n",
    "ax[1].set_title(f'{hydra_col} vs. {target}')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.1: Perform a linear regression with numpy and evaluate the model\n",
    "# --------------------------------------------------------------------\n",
    "\n",
    "target = 'pH_H2O_10cm' # d_inv_samps column name for target property\n",
    "fdem_col = 'HCP1.0' # d_inv_samps column name for FDEM dataset (inverted or ECa)\n",
    "\n",
    "# ******************************************************************** #\n",
    "\n",
    "# create columnname with name based on target variable\n",
    "col_name = f'predicted_{target}'\n",
    "\n",
    "# Ensure the dataset is clean and well-prepared\n",
    "ds_in.dropna(inplace=True)\n",
    "ds_in[fdem_col] = pd.to_numeric(ds_in[fdem_col], errors='coerce')\n",
    "ds_in[hydra_col] = pd.to_numeric(ds_in[hydra_col], errors='coerce')\n",
    "ds_in[target] = pd.to_numeric(ds_in[target], errors='coerce')\n",
    "\n",
    "# Extract features and target variables\n",
    "X = ds_in[fdem_col].values.reshape(-1, 1)\n",
    "y = ds_in[target].values\n",
    "\n",
    "# Sort the dataset based on the feature variable\n",
    "X_sorted = np.sort(X, axis=0)\n",
    "\n",
    "# numpy\n",
    "lin_fit = np.poly1d(np.polyfit(ds_in[fdem_col].values, \n",
    "                               ds_in[target].values, 1))\n",
    "ds_in[col_name] = lin_fit(ds_in[fdem_col].values)\n",
    "\n",
    "# Calculate R-squared for model using 'leave-one-out' cross-validation\n",
    "# implemented via scikit-learn (sklearn)\n",
    "\n",
    "# Calculate R-squared for linear model\n",
    "r2_linear_numpy = r2_score(y, lin_fit(X))\n",
    "\n",
    "# Cross-validation (leave-one-out)\n",
    "# --------------------------------\n",
    "# iterate over the dataset and calculate the MSE for each iteration using\n",
    "# the leave-one-out via Scikit-learn\n",
    "loo = LeaveOneOut()\n",
    "mse_scores_numpy = []\n",
    "for train_index, test_index in loo.split(X):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    lin_fit = np.poly1d(np.polyfit(X_train[:,0], y_train, 1))\n",
    "    y_pred_numpy_eval = lin_fit(X_test[:, 0])\n",
    "\n",
    "    # calculate the mean squared error for each iteration\n",
    "    mse_scores_numpy.append(mean_squared_error(y_test, y_pred_numpy_eval))\n",
    "\n",
    "# calculate the average mean squared error\n",
    "average_mse_numpy = np.mean(mse_scores_numpy)\n",
    "\n",
    "# Plotting\n",
    "fig, ax = plt.subplots(1,2, figsize=(15,7))\n",
    "\n",
    "# plot the linear regression model against input vs target data\n",
    "ax[0].scatter(ds_in[fdem_col], ds_in[target], \n",
    "              color='black', label='Observations')\n",
    "ax[0].plot(ds_in[fdem_col], ds_in[col_name], \n",
    "           label=f'Linear regression, R2 = {r2_linear_numpy:.3f}')\n",
    "ax[0].set_xlabel(fdem_col)\n",
    "ax[0].set_ylabel(target)\n",
    "ax[0].legend()\n",
    "\n",
    "# plot comparison of observed and predicted target property\n",
    "# add 1:1 line\n",
    "ax[1].plot([ds_in[target].min(), ds_in[target].max()], \n",
    "            [ds_in[target].min(), ds_in[target].max()], \n",
    "            color='red', alpha=0.3, linestyle='--')\n",
    "ax[1].scatter(ds_in[target], ds_in[col_name], s=200)\n",
    "# add sample ID labels\n",
    "for index, row in ds_in.iterrows():\n",
    "    ax[1].text(row[target], row[col_name], str(int(row['ID'])), \n",
    "            fontsize=10, ha='center', va='center', color='white')\n",
    "\n",
    "ax[1].set_xlabel(f'Observed {target}')\n",
    "ax[1].set_ylabel(f'Predicted {target}')\n",
    "ax[1].set_title('average MSE: {:.3f}'.format(average_mse_numpy))\n",
    "# Set ax[1] limits\n",
    "combined_min = min(ds_in[target].min(), ds_in[col_name].min())\n",
    "combined_max = max(ds_in[target].max(), ds_in[col_name].max())\n",
    "ax[1].set_xlim([combined_min-1, combined_max-1])\n",
    "ax[1].set_ylim([combined_min-1, combined_max+1])\n",
    "ax[1].grid()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform prediction for entire survey area\n",
    "\n",
    "In **_code cell 3.2_** you can apply a suited model to the entire FDEM dataset. You can specify the target and the data feature (FDEM dataset or inverted EC layer) to preform the prediction with. Make sure that you use the suited model (i.e., use the same input data as you trained your model on). \n",
    "In the cell, you can interpolate and export the predicted property map to a georeferenced raster, as well as to a .csv file.\n",
    "\n",
    "You can then import these data into your GIS and compare it to available information on your target property. \n",
    "\n",
    "_Sidenote: to change the plot colormaps, you find reference info on the [Matplotlib documentation site](https://matplotlib.org/stable/users/explain/colors/colormaps.html)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.2: Predict a target property based on the regression model from code cell 3.1\n",
    "# --------------------------------------------------------------------------\n",
    "\n",
    "# If needed, you can replace d_inv with another dataframe \n",
    "# The d_inv dataframe contains the full inverted (see code cell 0.1)\n",
    "dpred_in = d_inv.copy() \n",
    "\n",
    "# Set the dataframe column name that holds the EC values.For the inverted\n",
    "# EC values, the column names depend on the boundaries you have set.\n",
    "fdem_col = 'EC_0.60'\n",
    "\n",
    "# column name for the EC values you want to use for the prediction\n",
    "# SAME AS SPECIFIED FOR THE MODEL YOU CREATED IN 3.1\n",
    "target = target \n",
    "col_name = f'predicted_{target}' # column name for the predicted target property\n",
    "\n",
    "# Interpolate the predicted target property values to a grid and export\n",
    "cell_size = 0.25\n",
    "export_raster = True # if true the interpolated data will be exported as a geoTIFF\n",
    "export_csv = True # if true the interpolated data will be exported as a CSV\n",
    "\n",
    "# ******************************************************************** #\n",
    "# Run prediction based on the linear model from cell 3.1\n",
    "dpred_in[col_name] = lin_fit(dpred_in[fdem_col].values)\n",
    "\n",
    "# Interpolate results and plot the outcome\n",
    "cell_size = 0.25\n",
    "\n",
    "prediction_grid = interpolate(dpred_in['x'], dpred_in['y'], dpred_in[col_name], cell_size=cell_size)\n",
    "grid_name = f'{col_name}_grid'\n",
    "# Specify the grid extent for plotting with correct x-y coordinates\n",
    "extent = prediction_grid['extent']\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(figsize=(7,7))\n",
    "im = ax.imshow(prediction_grid['grid'], \n",
    "                origin='lower', \n",
    "                extent=(extent['x_min'],\n",
    "                        extent['x_max'],\n",
    "                        extent['y_min'],\n",
    "                        extent['y_max']),\n",
    "                cmap = 'copper_r'\n",
    "                )\n",
    "# Set limits to the plotting range based on data percentiles by \n",
    "# uncommenting the 4 lines below: \n",
    "\n",
    "# pmin = 2  # lower percentile\n",
    "# pmax = 98  # upper percentile \n",
    "# im.set_clim(np.percentile(data_grid['grid'].flatten()[~np.isnan(data_grid['grid'].flatten())], pmin),\n",
    "#         np.percentile(data_grid['grid'].flatten()[~np.isnan(data_grid['grid'].flatten())], pmax))\n",
    "\n",
    "ax.set_title(f\"{col_name}\")\n",
    "plt.colorbar(im, ax=ax)\n",
    "\n",
    "if export_raster:\n",
    "    # Export the interpolated data as a GeoTIFF\n",
    "    export_grid(prediction_grid, filename = grid_name)\n",
    "\n",
    "if export_csv:\n",
    "   # generate dataframe with only predicted values (col_name) and columns 'x', 'y', and the fdem_col\n",
    "    dpred_out = dpred_in[['x', 'y', fdem_col, col_name]]\n",
    "    dpred_out.to_csv(f'{col_name}.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "em_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
