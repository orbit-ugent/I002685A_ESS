{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ESS: Analysing and interpreting FDEM data - PC class 1 (29.04.2024)\n",
    "---\n",
    "# 0 - Introduction\n",
    "In this notebook you will explore FDEM data, evaluating the spatial variations and how the collected datasets relate to different soil volumes. By combining collected datasets in a GIS, you can explore how the observed variations relate to known soil information. In a last step, you will be able to evaluate the collected data in a more quantitative way by performing an inversion.\n",
    "\n",
    "This notebook consists of Markdown (text) cells, such as this one, and code cells like the one below. Each code cell is numbered for reference. The first code cell below is **_code cell 0.0_**. Running that cell and **_code cell 0.1_**  installs the required packages in Google Colaboratory, and subsequently import all required packages into the workspace.\n",
    "\n",
    "Throughout the notebook, in cases where you perform data analyses, the part of the code where you can modify variables, or write functions, always appears above a commented line of asterisks, like this: `# ******* `. If no asterisk line is present in a code cell, this means you can simply run the code cell without changing anything to get the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages as needed and import\n",
    "# -------------------------------------------- #\n",
    "\n",
    "import subprocess\n",
    "import importlib\n",
    "import sys\n",
    "\n",
    "# List of required packages\n",
    "required_packages = [\n",
    "    'IPython',\n",
    "    'ipywidgets',\n",
    "    'numpy',\n",
    "    'pandas',\n",
    "    'matplotlib',\n",
    "    'scipy',\n",
    "    'sklearn',\n",
    "    'geopandas',\n",
    "    'rasterio',\n",
    "    'shapely',\n",
    "    'emagpy',\n",
    "    'requests'\n",
    "]\n",
    "\n",
    "def check_and_install_packages(package_list):\n",
    "# Function to check for required packages and install them if not found\n",
    "# integrates functionality to operate in Jupyter environment (including \n",
    "# Google Colab) or standard IDE (VSCode, PyCharm, etc.)\n",
    "# ==================================================================== #\n",
    "    for package in package_list:\n",
    "        try:\n",
    "            importlib.import_module(package)\n",
    "        except ImportError:\n",
    "            print(f\"{package} not found, installing...\")\n",
    "            try:\n",
    "                # Check if in a Jupyter (IPython) environment\n",
    "                if 'get_ipython' in globals():\n",
    "                    print(\"Using Jupyter magic command to install.\")\n",
    "                    get_ipython().system(f'pip install {package}')\n",
    "                else:\n",
    "                    # Fallback to standard IDE installation method\n",
    "                    subprocess.run(\n",
    "                        [sys.executable, '-m', 'pip', 'install', package], \n",
    "                        check=True, \n",
    "                        capture_output=True\n",
    "                        )\n",
    "            except Exception as e:\n",
    "                print(f\"{package} not installed: {e}\")\n",
    "            # Try importing the package again after installation\n",
    "            importlib.import_module(package)\n",
    "\n",
    "check_and_install_packages(required_packages)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0.1: Import packages into workspace\n",
    "# -----------------------------------------------------\n",
    "'''\n",
    "Import the required modules to run all code in this notebook.\n",
    "'''\n",
    "# General utility modules\n",
    "import os\n",
    "import warnings\n",
    "import re\n",
    "import requests\n",
    "\n",
    "# Modules for geopunt data visualisation\n",
    "from IPython.display import HTML\n",
    "from IPython.core.display import display\n",
    "from ipywidgets import widgets, HBox\n",
    "\n",
    "# Data visualisation, manipulation, and analysis\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.path\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "from scipy.spatial import cKDTree\n",
    "import scipy.stats\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.interpolate import griddata\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score\n",
    "from scipy.optimize import root\n",
    "\n",
    "# Geospatial data manipulation and raster operations\n",
    "import geopandas as gpd\n",
    "import rasterio\n",
    "from rasterio.transform import from_origin\n",
    "from rasterio.warp import calculate_default_transform, reproject, Resampling\n",
    "from scipy.ndimage import gaussian_filter\n",
    "\n",
    "from shapely.geometry import Point\n",
    "\n",
    "# EMI 1D inversion package (emagpy)\n",
    "from emagpy import Problem\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Field dataset - Testfield Proefhoeve Bottelaere [Vijverhoek, Oosterzele]\n",
    "\n",
    "The key dataset you will work with in this notebook, is the frequency-domain electromagnetic induction (FDEM) dataset that was collected at our testsite on April 26, 2024. Data were collected with a Dualem-21HS instrument. Based on the soil map, soil texture in the test site ranges from sandy loam to clay. WRB soil groups are Cambisols (Eutric Stagnic Cambisols (Loamic)) and Stagnosols (Dystric Retic Stagnosols (Loamic)), both are generally Ruptic (referring to the occurrence of two different source materials, in this case aeolian sand/loess over Tertiary clay). You can explore the soil map, along with the WRB classification, in **_code cell 0.2_**.\n",
    "\n",
    "The data you have are collected with a FDEM instrument with the following specifications:\n",
    "- operating frequency: 9000 Hz\n",
    "- coil geometries:\n",
    "    1. three coil pairs in HCP mode with Rx at 0.5 m (HCP0.5), 1.0 m (HCP2.1), 2.0 m (HCP2.0) from Tx.\n",
    "    2. three coil pairs in PRP mode with Rx at 0.6 m (PRP0.6), 1.1 m (PRP1.1), 2.1 m (PRP2.1) from Tx.\n",
    "- output: QP data as LIN ECa [mS/m], IP data as field intensity [ppt].\n",
    "\n",
    "Table 1 below lists all columns of the FDEM datasets, and explains their datatype.\n",
    "\n",
    "> \n",
    ">|Column name|datatype|\n",
    ">|-----------|--------|\n",
    ">| *x* | easting [m]|\n",
    ">| *y* | northing [m]|\n",
    ">| *z* | elevation [m]|\n",
    ">| *t* | timestamp [s]|\n",
    ">| *HCP0.5* | 0.5 m HCP LIN ECa data [mS/m]|\n",
    ">| *PRP0.6* | 0.6 m PRP LIN ECa data [mS/m]|\n",
    ">| *HCP1.0* | 1.0 m HCP LIN ECa data [mS/m]|\n",
    ">| *PRP1.1* | 1.1 m PRP LIN ECa data [mS/m]|\n",
    ">| *HCP2.0* | 2.0 m HCP LIN ECa data [mS/m]|\n",
    ">| *PRP2.1* | 2.1 m PRP LIN ECa data [mS/m]|\n",
    ">| *HCP0.5_inph* | 0.5 m HCP inphase data [ppt]|\n",
    ">| *PRP0.6_inph* | 0.6 m PRP inphase data [ppt]|\n",
    ">| *HCP1.0_inph* | 1.0 m HCP inphase data [ppt]|\n",
    ">| *PRP1.1_inph* | 1.1 m PRP inphase data [ppt]|\n",
    ">| *HCP2.0_inph* | 2.0 m HCP inphase data [ppt]|\n",
    ">| *PRP2.1_inph* | 2.1 m PRP inphase data [ppt]|\n",
    ">\n",
    "> *Table 1: overview of FDEM data column names and the datatype these hold.*\n",
    ">*(The x and y coordinates are presented in meters Belge Lambert 72 (EPSG:31370), and the elevation (z) in meters above sea level.)* \n",
    "\n",
    "Basic processing has been performed, which entails:\n",
    "- accurate georeferencing and projection to Belge Lambert 1972 coordinate system,\n",
    "- removal of erroneous datapoints (e.g., standstill moments when the survey setup stopped for >5 seconds),\n",
    "- drift correction (of all IP and QP/ECa datasets).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0.2: Exploring test site geodata from 'geopunt.be'\n",
    "# --------------------------------------------------\n",
    "\n",
    "from IPython.display import HTML\n",
    "from IPython.core.display import display\n",
    "from ipywidgets import widgets, HBox\n",
    "\n",
    "airph = '<iframe src=\"https://www.geopunt.be/embed/d1fa24f5-a258-45b1-887d-8c1f6928a437\" width=\"500\" height=\"600\"></iframe>'\n",
    "soilm = '<iframe src=\"https://www.geopunt.be/embed/e7d14f05-9d72-4df1-b8ff-4a2b2a9311db\" width=\"500\" height=\"600\"></iframe>'\n",
    "\n",
    "AerialPhoto = widgets.HTML(airph)\n",
    "SoilMap = widgets.HTML(soilm)\n",
    "\n",
    "twinbox = HBox([AerialPhoto, SoilMap])\n",
    "display(twinbox)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading data\n",
    "\n",
    "These include:\n",
    "- FDEM_surveydata: full survey dataset of the Bottelare testfield;\n",
    "- FDEM_transect: extract of the survey dataset along a reference transect. You can use this to test analytical procedures, which you can then deploy across the full survey dataset;\n",
    "- sampling_data: locations of calibration and validation samples, alongside soil property information\n",
    "\n",
    "**All geospatial datasets are provided in Lambert 1972 coordinates** (EPSG:31370).\n",
    "At any point, if the dataframe structure is unclear, or you want to see the column names, you can use the built-in functions df.head() or df.columns. You can also simply download the .csv files and open these in excel or your preferred software.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0.3: Get dataset location\n",
    "# -------------------------\n",
    "# store dataset URL's as string variables\n",
    "FDEM_surveydata = 'https://users.ugent.be/~pjdsmedt/ESS2024/FDEM_280423.csv'\n",
    "FDEM_transect = 'https://users.ugent.be/~pjdsmedt/ESS2024/FDEM_transect_280423.csv'\n",
    "samples = 'https://users.ugent.be/~pjdsmedt/ESS2024/samples_combined23.csv' \n",
    "\n",
    "# URL for grid masking file\n",
    "blank_json = 'https://users.ugent.be/~pjdsmedt/ESS2024/blank.json'\n",
    "\n",
    "# Create dataframes from datasets\n",
    "'''\n",
    "Import datasets as dataframes\n",
    "-----------------------------\n",
    "    - df = dataframe with the full FDEM dataset\n",
    "    - dt = dataframe with the FDEM transect\n",
    "    - ds = datasframe with the sample data (including analytical data)\n",
    "    - blank = geojson (polygon) outlining survey extent\n",
    "'''\n",
    "df = pd.read_csv(FDEM_surveydata, sep=',', header=0)\n",
    "dt = pd.read_csv(FDEM_transect, sep=',', header=0)\n",
    "ds = pd.read_csv(samples, sep=',', header=0)\n",
    "blank_in = gpd.read_file(blank_json)\n",
    "blank = blank_in.to_crs('EPSG:31370')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pandas dataframe names\n",
    "\n",
    "As presented in the code cell above, all datasets are loaded as pandas dataframes. You can access the column data through the column names, as is the case throughout this notebook.\n",
    "If you want to quickly view the column names, just create a code cell, and type the dataframe name with the suffix *.colums* ( like this: `df.columns`) and run the cell, you can run **_code cell 0.4_** as example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0.4: Printing dataframe column names\n",
    "# ------------------------------------\n",
    "\n",
    "\"\"\" \n",
    "Dataframe names for reference.\n",
    "- df = dataframe with the full FDEM dataset\n",
    "- dt = dataframe with the FDEM transect\n",
    "- ds = datasframe with the sample data (including analytical data)\n",
    "\"\"\"\n",
    "\n",
    "df.columns # change the dataframe name to visualise its columns\n",
    "\n",
    "# ******************************************************************** #"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Interpolating and exporting data.\n",
    "\n",
    "In this notebook, you will work with geospatial data in vector (point) and raster format. To help you along the way to analyse and visualise data, in **_code cell 1.0_**, basic functions to interpolate and export data are given. You can call these functions anywhere in the notebook, as long as you run the following code cell first. \n",
    "When working in Google Colaboratory, running the export function anywhere in the notebook will store the exported geotif on your Google Drive (or in your IDE's working folder if you are working locally).  \n",
    "\n",
    "**!!! If you export two different datasets with the same filename, the first dataset will be overwritten !!!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.0: Functions for interpolating and exporting data\n",
    "# ----------------------------------------------------\n",
    "\n",
    "# a. Function to quickly interpolate a scatter dataset to a regular grid.\n",
    "# -----------------------------------------------------------------------\n",
    "\n",
    "def interpolate(x, y, z, cell_size, method='nearest', \n",
    "                smooth_s = 0, blank=blank):\n",
    "    \"\"\"\n",
    "    Interpolate scatter data to regular grid through selected interpolation \n",
    "    method (with scipy.interpolate for simple interpolation).\n",
    "\n",
    "    The output of this function is a Numpy array that holds the interpolated \n",
    "    data (accessed via `datagrid['grid']` in the cell below), alongside \n",
    "    the grid's cell size (`datagrid['cell_size']`) and the grid extent \n",
    "    (`datagrid['extent']`). The cell size and extent are needed to allow \n",
    "    exporting the interpolated data efficiently to a GeoTIF that can be \n",
    "    opened in any GIS software such as QGIS. \n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x : np.array\n",
    "        Cartesian GPS x-coordinates.\n",
    "\n",
    "    y : np.array\n",
    "        Cartesian GPS y-coordinates.\n",
    "\n",
    "    z : np.array\n",
    "        Data points to interpolate.\n",
    "\n",
    "    cell_size : float\n",
    "        Grid cell size (m).\n",
    "\n",
    "    method : str, optional\n",
    "        Scipy interpolation method ('nearest', 'linear', 'cubic' or 'IDW')\n",
    "\n",
    "    smooth_s : float, optional\n",
    "        Smoothing factor to apply a Gaussian filter on the interpolated grid.\n",
    "        If 0, no smoothing is performed. (Applying smoothing can result \n",
    "        in a loss of detail in the interpolated grid.)\n",
    "\n",
    "    blank : object\n",
    "        A blank object to mask (clip )interpolation beyond survey bounds.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    grid : np.array\n",
    "        Array of interpolated and masked grid containing:\n",
    "        - the interpolated grid (grid['grid'])\n",
    "        - the grid cell size (grid['cell_size'])\n",
    "        - the grid extent (grid['extent'])\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Define an evenly spaced grid over which the dataset values have to be \n",
    "    # interpolated\n",
    "    x_min = x.min()\n",
    "    x_max = x.max() + cell_size\n",
    "    y_min = y.min()\n",
    "    y_max = y.max() + cell_size\n",
    "    x_vector = np.arange(x_min, x_max, cell_size)\n",
    "    y_vector = np.arange(y_min, y_max, cell_size)\n",
    "    extent = (x_vector[0], x_vector[-1], y_vector[0], y_vector[-1])\n",
    "\n",
    "    xx, yy = np.meshgrid(x_vector, y_vector)\n",
    "    nx, ny = xx.shape\n",
    "    coords = np.concatenate((xx.ravel()[np.newaxis].T, \n",
    "                        yy.ravel()[np.newaxis].T), \n",
    "                        axis=1)\n",
    "\n",
    "    # Create a mask to blank grid outside surveyed area\n",
    "    boolean = np.zeros_like(xx)\n",
    "    boundaries = np.vstack(blank.loc[0, 'geometry'].exterior.coords.xy).T\n",
    "    bound = boundaries.copy()\n",
    "    boolean += matplotlib.path.Path(\n",
    "        bound).contains_points(coords).reshape((nx, ny))\n",
    "    boolean = np.where(boolean >= 1, True, False)\n",
    "    mask = np.where(boolean == False, np.nan, 1)\n",
    "    binary = np.where(boolean == False, 0, 1)\n",
    "    \n",
    "    # Fast (and sloppy) interpolation (scipy.interpolate)\n",
    "    if method in ['nearest','cubic', 'linear']:\n",
    "        # Interpolate \n",
    "        data_grid = griddata(\n",
    "            np.vstack((x, y)).T, z, (xx, yy), method=method\n",
    "            ) * mask\n",
    "    else:\n",
    "        print('define interpolation method')\n",
    "    \n",
    "    if smooth_s > 0:\n",
    "        data_grid = gaussian_filter(data_grid, sigma=smooth_s)\n",
    "\n",
    "    # Create a structured array with additional fields for coordinates and cell size\n",
    "    dtype = [\n",
    "        ('grid', data_grid.dtype, data_grid.shape),\n",
    "        ('cell_size', float),\n",
    "        ('extent', [\n",
    "            ('x_min', float), \n",
    "            ('x_max', float), \n",
    "            ('y_min', float), \n",
    "            ('y_max', float)\n",
    "            ])\n",
    "    ]\n",
    "    grid = np.array((data_grid, cell_size, extent), dtype=dtype)\n",
    "    \n",
    "    return grid\n",
    "  \n",
    "\n",
    "# b. Function to export an interpolated grid as a geotif.\n",
    "# -------------------------------------------------------\n",
    "\n",
    "def export_grid(grid_in, filename='georaster'): \n",
    "    \"\"\"\n",
    "    Interpolate scatter data to regular grid through selected interpolation \n",
    "    method (with scipy.interpolate for simple interpolation).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    grid_in : np.array\n",
    "        Array of interpolated and masked grid.\n",
    "\n",
    "    filename : str, optional\n",
    "        Name of the GeoTIFF (.tif) file (standard = 'gridded').\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "    !!! data are exported to the working directory as a GeoTIFF file.\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    #   Get grid properties\n",
    "    cell_size = grid_in['cell_size']\n",
    "    extent = grid_in['extent']\n",
    "    transform = from_origin(extent['x_min'], extent['y_min'], \n",
    "                                cell_size, -cell_size)\n",
    "\n",
    "    #   Prepare rasterio grid\n",
    "    grid_exp = grid_in['grid']\n",
    "    grid_exp[np.isnan(grid_exp)] = -99999\n",
    "    grid_exp = grid_exp.astype(rasterio.float32)\n",
    "    nx, ny = grid_exp.shape\n",
    "    grid_exp = np.flip(grid_exp, axis=0)\n",
    "\n",
    "    \n",
    "    #   Create an empty grid with correct name and coordinate system\n",
    "    with rasterio.open(\n",
    "        filename + '.tif',\n",
    "        mode='w',\n",
    "        driver='GTiff',\n",
    "        height=nx,\n",
    "        width=ny,\n",
    "        count=1,\n",
    "        dtype=str(grid_exp.dtype),\n",
    "        crs='EPSG:31370', #Lambert 1972 coordinates\n",
    "        transform=transform,\n",
    "        nodata=-99999\n",
    "    ) as dst:\n",
    "        dst.write(grid_exp, 1)\n",
    "\n",
    "    # Open the GeoTIFF file in read/write mode to flip the image vertically\n",
    "    with rasterio.open(filename + '.tif', mode='r+') as dst:\n",
    "        data = dst.read()\n",
    "        dst.write(data[0, ::-1], 1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example use of functions\n",
    "\n",
    "In the code cells below, you can use the `interpolate` function to interpolate a single dataset from the collected FDEM dataset.\n",
    "\n",
    "**_Code cell 1.1_** allows interpolating and plotting a single dataset. You can do this for all datasets, by specifying the data column through the `col` variable. If you run this notebook for the first time, the `col` variable will be set to `HCP1.0`. This will result in the interpolation of the ECa data collected with the 1.0 m HCP coil configuration. You can change it to any of the data column names of the FDEM survey dataset, shown in *Table 1*. The second variable to set is the grid cell size (`cell_size` variable), which specifies the resolution of the final grid in meters. You can export the interpolated dataset as a GeoTIF using the `export_grid` function for use in a GIS program, by setting te `export` variable to `True`. You can reuse these functions across the notebook, to interpolate datasets and export generated rasters.  \n",
    "\n",
    "**_Code cell 1.2_** is a more advanced example of how you an iteratively interpolate and plot all datasets from the FDEM survey. Running this cell provides an overview of all collected ECa and IP datasets.\n",
    "\n",
    "Take note that the interpolation algorithms implemented here have the primary purpose of quickly visualising data. These are very simple interpolators, from the [SciPy](https://docs.scipy.org/doc/scipy/tutorial/interpolate.html) package, that do not take geostatistical relationships or distances between points into account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.1: Example of interpolating and plotting a single FDEM dataset\n",
    "# ----------------------------------------------------------------\n",
    "\n",
    "# Specify the data column of which you want to interpolate the values\n",
    "col = 'HCP1.0'\n",
    "cell_size = 0.25 # raster cell size in meters\n",
    "\n",
    "export = False # set to True to export the interpolated grid as a GeoTIF\n",
    "f_name = '1m_HCP_ECa' # name of the exported GeoTIF file (in the example, 1m_HCP_ECa.tif will be the filename for the HCP1.0 dataset)\n",
    "\n",
    "# ******************************************************************** #\n",
    "\n",
    "# Interpolation function\n",
    "data_grid = interpolate(df['x'], df['y'], df[col], cell_size=cell_size)\n",
    "\n",
    "# Specify the grid extent for plotting with correct x-y coordinates\n",
    "extent = data_grid['extent']\n",
    "\n",
    "# Set units and colormap (cmap) for either IP or ECa data\n",
    "if 'inph' in col:\n",
    "        unit = 'IP [ppt]'\n",
    "        cmap = 'gray_r'\n",
    "else:\n",
    "        unit = 'ECa [mS/m]'\n",
    "        cmap = 'viridis_r'\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(7,7))\n",
    "im = ax.imshow(data_grid['grid'], \n",
    "                origin='lower', \n",
    "                extent=(extent['x_min'],\n",
    "                        extent['x_max'],\n",
    "                        extent['y_min'],\n",
    "                        extent['y_max']),\n",
    "                cmap = 'viridis_r'\n",
    "                )\n",
    "# Set limits to the plotting range based on data percentiles by \n",
    "# uncommenting the 4 lines below: \n",
    "\n",
    "# pmin = 2  # lower percentile\n",
    "# pmax = 98  # upper percentile \n",
    "# im.set_clim(np.percentile(data_grid['grid'].flatten()[~np.isnan(data_grid['grid'].flatten())], pmin),\n",
    "#         np.percentile(data_grid['grid'].flatten()[~np.isnan(data_grid['grid'].flatten())], pmax))\n",
    "\n",
    "ax.set_title(f\"{col} ({unit})\")\n",
    "plt.colorbar(im, ax=ax)\n",
    "\n",
    "# Export the grid as a GeoTIF\n",
    "\"\"\"\n",
    "You can set the exported file's name by modifying the f_name variable\n",
    "\"\"\"\n",
    "export_grid(data_grid, filename = f_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.2: Example on interpolating and plotting via loop and visualise all datasets\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "fig, axes = plt.subplots(4, 3, figsize=(20, 15))\n",
    "\n",
    "i = 0\n",
    "data_grids = {}\n",
    "for col in df.columns:\n",
    "    # Omit columns that do not contain measurement data\n",
    "    if col not in ['x','y','z','t']:\n",
    "        row, col_idx = divmod(i, 3)\n",
    "        ax = axes[row, col_idx]\n",
    "\n",
    "        # Set plotting properties for inphase and ECa data\n",
    "        if '_inph' in col:\n",
    "            cmap = 'gray_r'\n",
    "            unit = 'ppt'\n",
    "        else:\n",
    "            cmap = 'viridis_r'\n",
    "            unit = 'mS/m'\n",
    "\n",
    "        # Interpolate the survey dataset\n",
    "        data_grid = interpolate(df['x'], df['y'], df[col],\n",
    "                                        cell_size=0.5)\n",
    "        extent = data_grid['extent']\n",
    "        gridplot = data_grid['grid']\n",
    "\n",
    "        # Visualise the results\n",
    "        im = ax.imshow(\n",
    "                        gridplot, \n",
    "                        origin='lower', \n",
    "                        extent=(extent['x_min'],\n",
    "                                extent['x_max'],\n",
    "                                extent['y_min'],\n",
    "                                extent['y_max']),\n",
    "                        cmap = cmap\n",
    "                        )\n",
    "        ax.set_title(f\"{col} ({unit})\")\n",
    "        plt.colorbar(im, ax=ax)\n",
    "        i += 1\n",
    "df_data_grids = pd.DataFrame.from_dict(data_grids)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. FDEM data exploration\n",
    "\n",
    "### Evaluating variability in FDEM datasets\n",
    "\n",
    "Next, you can explore the variability in the FDEM data in a more in-depth way. In **_code cell 2.1_**, you have a function that allows generating the relative and cumulative sensitivity functions of the deployed coil pairs. This allows you to evaluate at which depth the coil geometry obtains its maximum sensitivity to changes in subsurface electrical conductivity (for the QP (or LIN ECa) response). \n",
    "Each of the conductivity maps (produced with a different coil configuration), represents the apparent electrical conductivity of a specific soil volume. The depth of investigation of each of these datasets can be approximated by the depth above which 70% of the signal response is obtained. Using the simplified sensitivity functions below, in **_code cell 2.2_** you can plot the cumulative sensitivities, along with a line that represents the 70% response threshold.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.1: Function to obtain the sensitivity of a given coil geometry\n",
    "# ----------------------------------------------------------------\n",
    "\n",
    "def lin_sens(geometry, maxdepth=3., n_int=100, sensor_height = 0):\n",
    "    \"\"\"\n",
    "    Calculate approximative cumulative and relative sensitivities based on\n",
    "    Keller & Frischknecht, 1966 and McNeill, 1980\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    geometry : str\n",
    "        coil geometry identifier, combining orientation ('HCP' or 'PRP'),\n",
    "        and Tx-Rx separation. 'HCP0.5'is thus a HCP orientation with a 0.5 m\n",
    "        coil separation.\n",
    "\n",
    "    maxdepth : float (optional, default=3.0)\n",
    "        Maximum depth to evaluate the sensitivity (m).\n",
    "\n",
    "    n_int : int (optional, default=100)\n",
    "        Number of depth intervals to evaluate the sensitivity.\n",
    "\n",
    "    sensor_height : float (optional, default=0)\n",
    "        Height of the sensor above the ground (m).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    rsens_QP : np.array\n",
    "        Array of relative QP sensitivities obtained over the evaluated depths.\n",
    "    \n",
    "    csens_QP : np.array, optional\n",
    "        Array of cumulative QP sensitivities obtained over the evaluated depths.\n",
    "\n",
    "    rsens_IP : np.array, optional\n",
    "        Array of relative IP sensitivities obtained over the evaluated depths.\n",
    "    \n",
    "    csens_IP : np.array, optional\n",
    "        Array of cumulative IP sensitivities obtained over the evaluated depths.\n",
    "\n",
    "    \"\"\"\n",
    "    # determine depth extent along which to evaluate sensitivity\n",
    "    depths = np.linspace(.0, maxdepth, n_int)\n",
    "\n",
    "    if 'inph' in geometry:\n",
    "        coil_spacing = float(geometry[3:6])\n",
    "    else: \n",
    "        coil_spacing = float(geometry[-3:])\n",
    "\n",
    "    # create empty arrays\n",
    "    csens_QP = np.empty_like(depths)\n",
    "    rsens_QP = np.empty_like(depths)\n",
    "    csens_IP = np.empty_like(depths)\n",
    "    rsens_IP = np.empty_like(depths)\n",
    "    \n",
    "    depth_ratio = (depths + sensor_height) / coil_spacing\n",
    "    if 'HCP' in geometry:\n",
    "        csens_IP = (1 - 8 * depth_ratio ** 2) / ((4 * (depth_ratio ** 2) + 1) ** (5 / 2))\n",
    "        rsens_IP = 12 * depth_ratio * (3 - 8 * depth_ratio ** 2) / (coil_spacing * ((4 * depth_ratio ** 2) + 1) ** (7 / 2))\n",
    "        csens_QP = 1 / ((4 * (depth_ratio ** 2) + 1) ** 0.5)\n",
    "        rsens_QP = 4 * depth_ratio / (coil_spacing * (4 * depth_ratio ** 2 + 1) ** (3 / 2))\n",
    "\n",
    "    if 'PRP' in geometry:\n",
    "        csens_IP = (6 * depth_ratio) / ((4 * (depth_ratio ** 2) + 1) ** 2.5)\n",
    "        rsens_IP = -(96 * (depth_ratio ** 2) - 6) / (coil_spacing * (4 * (depth_ratio ** 2) + 1) ** (7 / 2))\n",
    "        csens_QP = 1 - ((2 * depth_ratio) / ((4 * depth_ratio ** 2) + 1) ** 0.5)\n",
    "        rsens_QP = 2 / ((coil_spacing * (4 * depth_ratio ** 2) + 1) ** (3 / 2))\n",
    "\n",
    "    return rsens_QP, csens_QP, rsens_IP, csens_IP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.2: Evaluating FDEM data along the reference transect\n",
    "# ------------------------------------------------------\n",
    "\n",
    "\"\"\"\n",
    "Here you can compare plotted ECa data along the reference transect, \n",
    "and evaluate these at the sampling locations. In addition, you can \n",
    "evaluate the relative and cumulative sensitivity of the QP (LIN ECa) \n",
    "response of the deployed coil configurations. \n",
    "\"\"\"\n",
    "\n",
    "maximum_depth = 2 # maximum depth to which you want to invert the data\n",
    "\n",
    "# ******************************************************************** #\n",
    "\n",
    "fig, axes = plt.subplots(1,3, figsize=(21,7))\n",
    "ax1 = axes[0]\n",
    "ax2 = axes[1]\n",
    "ax3 = axes[2]\n",
    "for col in dt.columns:\n",
    "    if col not in ['x','y','z','t','pos'] and 'inph' not in col:\n",
    "        if 'PRP' in col:\n",
    "            ax1.plot(dt[col], linestyle = 'dashed', label = col)\n",
    "        else:\n",
    "            ax1.plot(dt[col], label = col)\n",
    "ax1.set_ylabel('ECa in mS/m')\n",
    "ax1.set_title('ECa data along reference transect (0 = North).')\n",
    "ax1.legend()\n",
    "n_int = 200\n",
    "depths = np.linspace(.0, maximum_depth, n_int)\n",
    "for col in dt.columns:\n",
    "    if col not in ['x','y','z','t','pos'] and 'inph' not in col:\n",
    "        rsens_QP, csens_QP, rsens_IP, csens_IP = lin_sens(col, maximum_depth, n_int)\n",
    "        if 'PRP' in col:\n",
    "            ax2.plot(rsens_QP, depths, linestyle = 'dashed', label=col)\n",
    "            ax3.plot(csens_QP, depths, linestyle = 'dashed', label=col)\n",
    "        else:\n",
    "            ax2.plot(rsens_QP, depths, label=col)\n",
    "            ax3.plot(csens_QP, depths, label=col)\n",
    "\n",
    "ax2.invert_yaxis()\n",
    "ax2.set_ylim(maximum_depth, 0)\n",
    "ax2.xaxis.tick_top()\n",
    "ax2.xaxis.set_label_position('top')\n",
    "ax2.set_xlabel('Relative QP Sensitivity [-]')\n",
    "ax2.set_ylabel('Depth [m]')\n",
    "ax2.legend()\n",
    "\n",
    "ax3.axvline(x=0.3, color='k', linestyle='-', linewidth=0.5, label='70% sensitivity')\n",
    "ax3.invert_yaxis()\n",
    "ax3.set_ylim(maximum_depth, 0)\n",
    "ax3.xaxis.tick_top()\n",
    "ax3.xaxis.set_label_position('top')\n",
    "ax3.set_xlabel('Cumulative QP Sensitivity [-]')\n",
    "ax3.set_ylabel('Depth [m]')\n",
    "ax3.legend()\n",
    "\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring data in QGIS\n",
    "\n",
    "#### <ins> Loading FDEM data and the Flemish Soil Map in QGIS <ins>\n",
    "\n",
    "With the above functions, you are all set to start exploring the FDEM data. If you have exported one or more of the FDEM datasets as geotifs, you can open these in a GIS (instructions here are for [QGIS](https://www.qgis.org/en/site/)) and compare the observed electromagnetic variations to known soil information.\n",
    "In QGIS, and create a new project (Ctrl+N, or by using the menus). Set the coordinate reference system to Belgian Lambert 1972 (EPSG:31370).\n",
    "\n",
    "Raster layers can be added by pressing Ctrl+Shift+R, or navigating to the `Layer > Add Layer > Add Raster Layer` menu item. \n",
    "You can also add the delimited text file (.csv-file) containing the sample data by pressing Ctrl+Shift+T or navigating to the `Layer > Add Layer > Add Delimited Text Layer` menu item. Make sure that you specific the columns with x- and y- coordinates correctly (X field = `x`; Y field = `y`).\n",
    "\n",
    "You can add the Flemish soil map to your project by adding the WMS layer. (Alternatively, you can also do this via the Geoloket plugin if you have this installed.)\n",
    "Do this by pressing CTRL-Shift-W (or via `Layer>Add Layer>Add WMS/WMTS Layer`). In the popup box you create a new service connection by pressing 'New'. You can enter a name to identify the layer (e.g. 'SoilMap'), and in the URL you copy this address: https://www.dov.vlaanderen.be/geoserver/bodemkaart/bodemtypes/wms?version=1.3.0&request=GetCapabilities&service=wms \n",
    "\n",
    "Then press 'Connect' and you should see the different layers in this WMS repository. If you select 1 -- bodemtypes, and Add this (click the button below), you have added the soil map to your project. You can explore the other layers in the WMS as well. Mainly the Digital Soil Map (1) and the drainage class map (2) are relevant for this study. \n",
    "\n",
    "The names are indicated in Dutch, but here you have them translated:\n",
    "\n",
    "1. soil types - Digital Soil Map of the Flemish Region ('bodemtypes')\n",
    "2. drainage classes (labeled)\n",
    "3. soil series (labeled)\n",
    "4. drainage classes (unlabeled)\n",
    "5. soil series (simplified legend)\n",
    "6. soil series (contours)\n",
    "7. soil series (unlabeled)\n",
    "\n",
    "#### <ins> Loading modelled soil property data based on the Flemish Soil Map in QGIS <ins>\n",
    "\n",
    "Alongside the Flemish soil map, the Flemish governement provides specific soil property information, that is modeled based on the soil map and point data ([more info here](https://dov.vlaanderen.be/page/virtuele-bodemanalyse)). These datasets provide quantitative information on soil properties (texture, organic matter content, ...) for different soil depths. You can import these datasets as WMS layers in QGIS as well via this URL: https://oefen.dov.vlaanderen.be/geoserver/bdbstat/wms?version=1.3.0 , following the same procedure as above. When loading the WMS data, you can select different soil property maps (see the screenshot below). \n",
    "\n",
    "\n",
    "<img src='https://users.ugent.be/~pjdsmedt/ESS2024/base/mod_wms-01.png' width=\"800\">\n",
    "\n",
    "If you select one with the suffix 'basisdata_bodemkartering', you can select model data for different depths, which you can compare with FDEM data representative for different soil volumes or depths (see section 3. FDEM data inversion).\n",
    "\n",
    "By evaluating these WMS datasets, you can compare the spatial patterning in the geophysical data to the information from the soil map, and to the sample information. You can use this QGIS project to combine and visualise all data from your project."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. FDEM data inversion\n",
    "\n",
    "Until now, you have worked with raw FDEM data. The QP output of the instrument is given as LIN ECa data, apparent EC data that consider a simple, linear relationship between the subsurface EC, and the QP instrument output. Hereby, the EC of all materials influencing the measurement is considered to be uniform, hence the 'apparent' nature of the data.\n",
    "  \n",
    "To evaluate the 'true' conductivity of the subsurface, we have to perform an inversion. In the following code cells, you can perform an inversion on the available FDEM datasets. More specifically, you can invert the ECa data layers collected during the survey, to model the subsurface conductivity. Through this process, you can create vertical sections (across the provided reference transect), and depth slices for specific depth ranges\n",
    "  \n",
    "To perform the inversion, we use the open-source EM modelling package [EMagPy](https://pypi.org/project/emagpy/), developed by [McLachlan et al. 2021](https://www.sciencedirect.com/science/article/pii/S0098300420305513). The sources code for this package is available on [GitHub](https://github.com/hkexgroup/emagpy/tree/master/src/emagpy).\n",
    "  \n",
    "All inversion procedures are driven by a forward model. As discussed, the forward model allows predicting the response obtained with a specific geophysical instrument used in a specific configuration, given an assumed subsurface model, described by a set of parameters. The underlying model builds on the sensitivities plotted in code block 1.2, only it integrates the cumulative sensitivity (CS) functions that describe how much of the total QP response (or, the ECa) of the instrument can be attributed to EC variations at different depths.\n",
    "  \n",
    "These CS functions assume that the sensitivity of the instrument depends only on the depth and the used coil configuration, and is independent of the subsurface EC and the instrument's operating frequency. More comprehensive models can be deployed as well in EMagPy, but these are beyond the scope of this exercise.\n",
    "\n",
    "In **_code cell 3.0_**, you will perform an example inversion along the reference transect. The EMagPy program reads from .csv-files, so the dataset path is the URL that is specified at the beginning of this notebook (stored in the `FDEM_transect` variable). \n",
    "You will implement the inversion in a very basic way. The initial parameter you will set is:\n",
    "- depths0: initial depths in meters of the bottom of each layer to be modelled. The bottom layer boundary is set to infinity (Numpy array with n depths).\n",
    "\n",
    "By loading the .csv files, EMagPy automatically recognises the data columns and coil configurations, and makes use of all ECa datasets to perform the inversion.\n",
    "\n",
    "All parameters of the inversion have been preset like this:\n",
    "> `transect.invert(forwardModel='CS', alpha=0.13, njobs=-1)` \n",
    "\n",
    "This includes the forward model to use (the cumulative sensitivity), and an alpha parameter that smooths the inversion.\n",
    "\n",
    "(<i>You can evaluate the use of different forward models by changing the > `forwardModel` argument in the function to 'FSlin' (which implements the full solution of Maxwell's equations, with LIN approximation) or with 'FSeq' (which implements the full solution of Maxwell's equations, without the approximation). These models are more robust, but come at a higher computational cost. A practical comparison of these procedures can be found [here](https://hkex.gitlab.io/emagpy/gallery/nb_paper-cs-vs-fs.html)<i>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.0: Performing an inversion along the reference transect\n",
    "# ----------------------------------------------------------\n",
    "# Set the data path\n",
    "transect_path = FDEM_transect\n",
    "\n",
    "# Determine the starting model parameters\n",
    "\"\"\"\n",
    "starting depths* are generated here with np.arange** alternatively, you \n",
    "can manually create an array of depth boundary values (e.g., depths = [0.5,0.1],\n",
    "for a 3-layer model with layer boundaries at 0.5 m and 1.0 m. \n",
    "\n",
    "* the minimum boundary depth has to be larger than 0\n",
    "\n",
    "** np.arange creates a 1D array of evenly spaced values. In the example below\n",
    "this is a 1D array of from 0.10 to 10, with a value interval of 0.1\n",
    "\"\"\"\n",
    "\n",
    "# Inversion interface depths from 0 - 2 m, with a 0.1 m interval\n",
    "min_depth = 0.10\n",
    "max_depth = 2.10\n",
    "interval_m = 0.10\n",
    "depths_in = np.arange(0.10, max_depth, interval_m) # np\n",
    "\n",
    "# # Specific inversion interface depths\n",
    "# depths_in = [0.2, 0.5, 1.0, 1.5, 2.0] # manual interface depths in meters\n",
    "\n",
    "# # Inversion interface depths from 0 - 2 m, with a logarithmic interval\n",
    "# n_log_spaces = 10\n",
    "# depths_in = np.logspace(np.log10(min_depth), \n",
    "#                         np.log10(max_depth), \n",
    "#                         n_log_spaces) # log\n",
    "\n",
    "\"\"\"\n",
    "Defining starting conductivities is optional, but if you remove (or comment)\n",
    "the line above, you have to remove that argument from the .setInit method \n",
    "below as `transect.setInit(depths0=depths_in)`.\n",
    "\"\"\"\n",
    "\n",
    "# ******************************************************************** #\n",
    "\n",
    "# Create an inversion problem object to solve with EMagPy\n",
    "transect = Problem()\n",
    "\n",
    "data_path = transect_path # path to the FDEM transect data\n",
    "# Create a survey object that stores the survey data (data_path) and \n",
    "# incorporates the instrument operating frequency (freq), the instrument height\n",
    "# (hx), and the unit of the conductivity data to be evaluated (unit)\n",
    "\n",
    "transect.createSurvey(data_path, freq=9000,hx=0.165,unit='ECa')\n",
    "transect.setInit(depths0=depths_in)\n",
    "\n",
    "# Run the inversion\n",
    "transect.invert(forwardModel='CS', alpha=0.13, njobs=-1)\n",
    "print('   Finished inversion')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting and exporting results <ins>along the reference transect<ins>\n",
    "\n",
    "By running **_code cell 3.1_** you can plot the inversion results, and put these in a dataframe. \n",
    "Running the cell also exports the inversion results as a csv-file (you can modify the title if wanted). From the dataframe, you can then extract a single profile and plot it. In **_code cell 3.2_** this is done for a single position along the transect. You can specify the position (in meters) by changing the variable `transect_position`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.1: Plot the inversion results and put outcomes into a pandas dataframe\n",
    "# ------------------------------------------------------------------------\n",
    "\n",
    "csv_filename = 'inverted_transect.csv'\n",
    "\n",
    "# ******************************************************************** #\n",
    "\n",
    "# Plot inversion outcomes down to a max depth of 2 m, and plotting the data\n",
    "# based on their true coordinates along the transect (dist=True).\n",
    "\n",
    "fig, ax_t = plt.subplots(figsize=(20,7))\n",
    "transect.showResults(dist=True, ax=ax_t) \n",
    "ax_t.set_title('Inversion results along the reference transect (0 m = north)')\n",
    "\n",
    "# Extracting the values from the first row of the transect.depths[0] array\n",
    "depth_values = transect.depths[0][0]\n",
    "\n",
    "# Creating the custom column names for layer_cols\n",
    "layer_cols = ['EC_{:.2f}'.format(d) for d in depth_values] + ['EC_end']\n",
    "\n",
    "# Combining the data from the 'x', 'y' columns and the transect.models[0] array\n",
    "data = np.c_[transect.surveys[0].df[['x', 'y']].values, transect.models[0]]\n",
    "\n",
    "# Creating the final dataframe with the desired column names\n",
    "dt_inv = pd.DataFrame(data, columns=['x', 'y'] + layer_cols)\n",
    "dt_inv['pos'] = dt['pos']\n",
    "# ----------------------------------------------------------------------\n",
    "\n",
    "# Export the dataframe as a csv-file\n",
    "dt_inv.to_csv(csv_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.2: Plotting a single inverted profile based on its position.\n",
    "# --------------------------------------------------------------\n",
    "\n",
    "# Select the row index for which you want to plot the data\n",
    "\n",
    "transect_position = 30  # profile position along the transect in meters\n",
    "                        # North = 0\n",
    "\n",
    "# ******************************************************************** #\n",
    "# Extract the data for the selected position along the transect\n",
    "closest_index = (dt_inv['pos'] - transect_position).abs().idxmin()\n",
    "row_data = dt_inv.loc[closest_index, layer_cols].values\n",
    "\n",
    "# Extract the depth values (excluding the 'EC_end' column)\n",
    "depth_values = [float(col[3:]) for col in layer_cols[:-1]]\n",
    "\n",
    "# Add the ending depth value (assuming equal spacing between depth values)\n",
    "depth_values.append(depth_values[-1] + (depth_values[-1] - depth_values[-2]))\n",
    "\n",
    "# Create the plot\n",
    "plt.figure()\n",
    "plt.step(row_data, depth_values)\n",
    "plt.xlabel('EC [mS/m]')\n",
    "plt.ylabel('Depth')\n",
    "plt.title('EC values at {} m on the transect'.format(transect_position))\n",
    "plt.gca().invert_yaxis()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelling EC variations <ins>at sampled locations<ins>\n",
    "\n",
    "By running **_code cell 3.3_** you can perform an inversion on the FDEM data collected at the locations where you took invasive samples. You can set a specific depth extent for the subsurface layers. Instead of modelling the conductivity for each 10 cm layer as in the transect inversion, try setting the layer boundaries based on your prior knowledge of the soil buildup, instead of modelling the conductivity for 10 cm thick layers as in the example above.\n",
    "This prior knowledge can be derived from the samples you took on the field, most importantly the soil profile descriptions and the downhole conductivity measurements.\n",
    "\n",
    "You will instantly plot the results, and write the outcome to a pdf. If you are happy with the results, and want to have these data in a csv file run **_code cell 3.4_** to export these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.3: Inversion at the sampling locations\n",
    "# -----------------------------------------\n",
    "\n",
    "# Determine the inversion parameters (layer boundary depths & conductivities)\n",
    "'''\n",
    "Here is an example for a 2 layer model with boundaries at 0.5 and 1 m. \n",
    "You can adjust de starting depths array (`depths_in`). Try creating a \n",
    "3-layer model based on the observations you made on the field\n",
    "'''\n",
    "\n",
    "#depths_in = [0.6,1.0] \n",
    "depths_in = np.arange(0.10, 1.75, 0.10) \n",
    "\n",
    "# Set pdf name for exporting plot\n",
    "pdf_name = 'Modelled_EC_profiles.pdf'\n",
    "\n",
    "# ******************************************************************** #\n",
    "# Create an inversion problem object to solve with EMagPy\n",
    "\n",
    "sampling_path = samples\n",
    "sample_transect = Problem()\n",
    "sample_transect.createSurvey(sampling_path, freq=9000,hx=0.165,unit='ECa')\n",
    "sample_transect.setInit(depths0=depths_in)\n",
    "\n",
    "# Run the inversion\n",
    "sample_transect.invert(forwardModel='CS', alpha=0.23, njobs=-1)\n",
    "print('   Finished inversion')\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# Plotting and exporting the EC profiles figure as a pdf\n",
    "# ----------------------------------------------------------------------\n",
    "\n",
    "# Putting the results in a dataframe, and concatenate with analytical data.\n",
    "depth_values = sample_transect.depths[0][0]\n",
    "layer_cols = ['EC_{:.2f}'.format(d) for d in depth_values] + ['EC_end']\n",
    "data = np.c_[sample_transect.surveys[0].df[['x', 'y']].values, \n",
    "             sample_transect.models[0]]\n",
    "\n",
    "ds_inv = pd.DataFrame(data, columns=['x', 'y'] + layer_cols)\n",
    "selected_columns_ds = pd.concat([ds.iloc[:, 2:8], ds.iloc[:, 17:]], axis=1)\n",
    "ds_all = pd.concat([ds_inv, selected_columns_ds], axis=1)\n",
    "ds_all = ds_all.dropna()\n",
    "\n",
    "unique_sample_ids = ds_all['ID'].unique()\n",
    "\n",
    "# Calculate number of rows and columns for the subplot\n",
    "subplot_rows = 3\n",
    "subplot_cols = 5\n",
    "\n",
    "# Get axis limits\n",
    "global_x_min = ds_all[layer_cols].min().min()\n",
    "global_x_max = ds_all[layer_cols].max().max()\n",
    "\n",
    "\n",
    "# Create the subplots\n",
    "fig, axes = plt.subplots(subplot_rows, subplot_cols, figsize=(15, 9))\n",
    "fig.subplots_adjust(hspace=0.5, wspace=0.3)\n",
    "\n",
    "# Loop through each sample ID and plot the EC values\n",
    "for i, sample_id in enumerate(unique_sample_ids):\n",
    "    # Get row index for the current sample ID\n",
    "    row_index = ds_all.loc[ds_all['ID'] == sample_id].index[0]\n",
    "    row_data = ds_all.loc[row_index, layer_cols].values\n",
    "\n",
    "    # Extract depth values\n",
    "    depth_values = [float(col[3:]) for col in layer_cols[:-1]]\n",
    "    depth_values.append(depth_values[-1] + (depth_values[-1] - depth_values[-2]))\n",
    "\n",
    "    # Get the current subplot axis\n",
    "    ax = axes[i // subplot_cols, i % subplot_cols]\n",
    "\n",
    "    # Plot the data\n",
    "    ax.step(row_data, depth_values)\n",
    "    ax.set_xlabel('EC [mS/m]')\n",
    "    ax.set_ylabel('Depth')\n",
    "    ax.set_title(f'Sample {int(sample_id)}')\n",
    "    ax.set_xlim(global_x_min, global_x_max)\n",
    "    ax.invert_yaxis()\n",
    "    ax.grid(True)\n",
    "\n",
    "fig.suptitle('Modelled EC profiles at sampling locations', fontsize=14)\n",
    "\n",
    "# Save the plot as a PDF file\n",
    "with PdfPages(pdf_name) as pdf:\n",
    "    pdf.savefig()\n",
    "\n",
    "# Show the subplots\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.4: Export as a csv-file\n",
    "csv_filename = 'inverted_samples.csv'\n",
    "ds_all.to_csv(csv_filename)\n",
    "\n",
    "# ******************************************************************** #"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelling EC variations <ins>for the entire survey dataset<ins>\n",
    "\n",
    "After evaluating the outcomes of the inversion conducted in the previous cells, you can apply the inversion to the entire survey dataset. Hereby, you can target specific layers in the subsurface, and model their electrical conductivity. This follows the same flow as in the previous code cells, only now you will not visualise the data along a transect (vertical slice), but you will create horizontal EC slices that represent the electrical conductivities of the layers you define by setting the model depths.\n",
    "\n",
    "Considering the availability of sampling information from the upper part of the soil, your main goal is to predict the spatial variability of your target properties across the entire survey area. \n",
    "\n",
    "In **_Code cell 3.5_** you can perform the inversion. Set the boundaries of the layers for which you want to predict the electrical conductivity at the desired depth. Base the depth boundary on the inversion results you obtained above, and on your prior knowledge of the subsurface (boreholes, downhole-ec data). In the example, the depths are similar as the initial examples above (`depths_in = [0.6,1.0]`). <ins>The results are exported as a .csv-file of which you can set the name.<ins>\n",
    "\n",
    "By running **_Code cell 3.6_**, you can visualise all depth slices that you have created. <ins>By setting the export condition (`export_rasters`) to `True`, you can export these slices<ins> as GeoTIFs using the interpolation and export functions from code cell 1.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.5: Inverting the full survey dataset\n",
    "# --------------------------------------\n",
    "''' \n",
    "runtime when creating 3-layer model: ca. 10 mins in Google Colaboratory\n",
    "'''\n",
    "depths_in = [0.6,1.0] \n",
    "csv_filename = 'inverted_survey.csv'\n",
    "\n",
    "# ******************************************************************** #\n",
    "\n",
    "file_path = FDEM_surveydata\n",
    "survey = Problem()\n",
    "survey.createSurvey(file_path, freq=9000,hx=0.165)\n",
    "survey.setInit(depths0=depths_in)\n",
    "survey.invert(forwardModel='CS',\n",
    "              method='L-BFGS-B',\n",
    "              alpha=0.23,njobs=-1)\n",
    "\n",
    "# Putting the results in a dataframe, and concatenate with analytical data.\n",
    "depth_values = survey.depths[0][0]\n",
    "layer_cols = ['EC_{:.2f}'.format(d) for d in depth_values] + ['EC_end']\n",
    "data = np.c_[survey.surveys[0].df[['x', 'y']].values, \n",
    "             survey.models[0]]\n",
    "\n",
    "inv_df = pd.DataFrame(data, columns=['x','y'] + layer_cols)\n",
    "\n",
    "# Export to csv\n",
    "inv_df.to_csv(csv_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.6: Interpolate, plot and export the inversion results\n",
    "# -------------------------------------------------------\n",
    "\n",
    "# Set the cell_size\n",
    "cell_size = 0.25\n",
    "\n",
    "# Exporting condition: if set to true, you export the final rasters\n",
    "# as GeoTIFs.\n",
    "export_rasters = True\n",
    "\n",
    "# ******************************************************************** #\n",
    "\n",
    "# Calculate the number of rows and columns for subplots based on the number of layer_cols\n",
    "n = len(layer_cols)\n",
    "ncols = int(np.ceil(np.sqrt(n)))\n",
    "nrows = int(np.ceil(n / ncols))\n",
    "\n",
    "# Calculate the global minimum and maximum values across all datasets\n",
    "global_min = min([inv_df[col].min() for col in layer_cols])\n",
    "global_max = max([inv_df[col].max() for col in layer_cols])\n",
    "\n",
    "# Create a figure with subplots\n",
    "fig, axes = plt.subplots(nrows, ncols, figsize=(20, 20))\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Iterate over the layer columns, interpolate the data, and plot the results\n",
    "for i, col in enumerate(layer_cols):\n",
    "    data_grid = interpolate(inv_df['x'], inv_df['y'], inv_df[col], cell_size)\n",
    "    extent = data_grid['extent']\n",
    "    im = axes[i].imshow(data_grid['grid'], origin='lower', \n",
    "                        extent=(extent['x_min'],\n",
    "                                extent['x_max'],\n",
    "                                extent['y_min'], \n",
    "                                extent['y_max']),\n",
    "                                cmap = 'viridis_r',\n",
    "                                vmin=global_min, vmax=global_max)\n",
    "    # Extract float values from column names\n",
    "    float_value = float(re.findall(r\"(\\d+\\.\\d+)\", col)[0]) if re.findall(r\"(\\d+\\.\\d+)\", col) else None\n",
    "    \n",
    "    # Set the title\n",
    "    if col == 'EC_end':\n",
    "        title = 'EC bottom layer'\n",
    "    elif i == 0:\n",
    "        title = f'EC 0 - {float_value:.2f} m'\n",
    "    else:\n",
    "        prev_float_value = float(re.findall(r\"(\\d+\\.\\d+)\", layer_cols[i-1])[0])\n",
    "        title = f'EC {prev_float_value:.2f} - {float_value:.2f} m'\n",
    "    \n",
    "    axes[i].set_title(title)\n",
    "    fig.colorbar(im, ax=axes[i])\n",
    "\n",
    "    # Export to GeoTIF if wanted\n",
    "    if export_rasters:\n",
    "        export_grid(data_grid, filename=title)\n",
    "\n",
    "# Remove any unused subplots\n",
    "for i in range(n, len(axes)):\n",
    "    axes[i].axis('off')\n",
    "\n",
    "# Display the figure\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "64741f101c4c61109c751d1cf312fb9d03f24ebf08ef89d3abcfdde998172cbb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
